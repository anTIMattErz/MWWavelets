{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NE Climate Database\n",
    "### MWWavelets\n",
    "This file collects all the methods used in applying the wavelet transforms to the database created for the NE Climate wavelet project. It first constructs the NE Climate database, then defines all of the methods for applying the transforms, along with various other helper sections for creating noisy data and adding the COI's on plots. Statistical testing section is included, but is not applicable to the current data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint 1\n",
      "checkpoint 2\n",
      "Completed Initilization of StateID Object:  62658\n",
      "Completed Initilization of StateID Object:  63207\n",
      "Completed Initilization of StateID Object:  67970\n",
      "Completed Initilization of StateID Object:  68138\n",
      "Completed Initilization of StateID Object:  170100\n",
      "Completed Initilization of StateID Object:  170814\n",
      "Completed Initilization of StateID Object:  171628\n",
      "Completed Initilization of StateID Object:  172426\n",
      "Completed Initilization of StateID Object:  172765\n",
      "Completed Initilization of StateID Object:  173046\n",
      "Cleaning at ID:  173944\n",
      "Year: 1901   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  173944\n",
      "Year: 1901   Value: -9999.0\n",
      "Set to:  0\n",
      "Completed Initilization of StateID Object:  173944\n",
      "Completed Initilization of StateID Object:  174566\n",
      "Completed Initilization of StateID Object:  175304\n",
      "Completed Initilization of StateID Object:  176905\n",
      "Cleaning at ID:  176937\n",
      "Year: 1900   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  176937\n",
      "Year: 1900   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  176937\n",
      "Year: 1900   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  176937\n",
      "Year: 1900   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  176937\n",
      "Year: 1900   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  176937\n",
      "Year: 1900   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  176937\n",
      "Year: 1900   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  176937\n",
      "Year: 1900   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  176937\n",
      "Year: 1900   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  176937\n",
      "Year: 1900   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  176937\n",
      "Year: 1900   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  176937\n",
      "Year: 1900   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  176937\n",
      "Year: 1901   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  176937\n",
      "Year: 1901   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  176937\n",
      "Year: 1901   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  176937\n",
      "Year: 1901   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  176937\n",
      "Year: 1901   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  176937\n",
      "Year: 1901   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  176937\n",
      "Year: 1901   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  176937\n",
      "Year: 1901   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  176937\n",
      "Year: 1901   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  176937\n",
      "Year: 1901   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  176937\n",
      "Year: 1901   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  176937\n",
      "Year: 1901   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  176937\n",
      "Year: 1901   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  176937\n",
      "Year: 1901   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  176937\n",
      "Year: 1901   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  176937\n",
      "Year: 1901   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  176937\n",
      "Year: 1901   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  176937\n",
      "Year: 1901   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  176937\n",
      "Year: 1901   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  176937\n",
      "Year: 1901   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  176937\n",
      "Year: 1901   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  176937\n",
      "Year: 1901   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  176937\n",
      "Year: 1901   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  176937\n",
      "Year: 1901   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  176937\n",
      "Year: 1901   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  176937\n",
      "Year: 1901   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  176937\n",
      "Year: 1902   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  176937\n",
      "Year: 1902   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  176937\n",
      "Year: 1902   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  176937\n",
      "Year: 1902   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  176937\n",
      "Year: 1902   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  176937\n",
      "Year: 1902   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  176937\n",
      "Year: 1902   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  176937\n",
      "Year: 1902   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  176937\n",
      "Year: 1902   Value: -9999.0\n",
      "Set to:  0\n",
      "Completed Initilization of StateID Object:  176937\n",
      "Completed Initilization of StateID Object:  179891\n",
      "Completed Initilization of StateID Object:  190120\n",
      "Completed Initilization of StateID Object:  190535\n",
      "Completed Initilization of StateID Object:  190736\n",
      "Cleaning at ID:  193213\n",
      "Year: 1900   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  193213\n",
      "Year: 1900   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  193213\n",
      "Year: 1900   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  193213\n",
      "Year: 1900   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  193213\n",
      "Year: 1901   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  193213\n",
      "Year: 1901   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  193213\n",
      "Year: 1901   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  193213\n",
      "Year: 1901   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  193213\n",
      "Year: 1901   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  193213\n",
      "Year: 1901   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  193213\n",
      "Year: 1901   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  193213\n",
      "Year: 1902   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  193213\n",
      "Year: 1902   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  193213\n",
      "Year: 1902   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  193213\n",
      "Year: 1902   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  193213\n",
      "Year: 1902   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  193213\n",
      "Year: 1902   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  193213\n",
      "Year: 1902   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  193213\n",
      "Year: 1902   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  193213\n",
      "Year: 1902   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  193213\n",
      "Year: 1903   Value: -9999.0\n",
      "Set to:  0\n",
      "Completed Initilization of StateID Object:  193213\n",
      "Completed Initilization of StateID Object:  194105\n",
      "Completed Initilization of StateID Object:  195246\n",
      "Completed Initilization of StateID Object:  196486\n",
      "Completed Initilization of StateID Object:  196681\n",
      "Completed Initilization of StateID Object:  196783\n",
      "Completed Initilization of StateID Object:  198367\n",
      "Completed Initilization of StateID Object:  198757\n",
      "Completed Initilization of StateID Object:  199316\n",
      "Completed Initilization of StateID Object:  270706\n",
      "Completed Initilization of StateID Object:  272174\n",
      "Cleaning at ID:  272999\n",
      "Year: 1900   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  272999\n",
      "Year: 1900   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  272999\n",
      "Year: 1900   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  272999\n",
      "Year: 1900   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  272999\n",
      "Year: 1900   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  272999\n",
      "Year: 1900   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  272999\n",
      "Year: 1900   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  272999\n",
      "Year: 1900   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  272999\n",
      "Year: 1900   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  272999\n",
      "Year: 1900   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  272999\n",
      "Year: 1901   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  272999\n",
      "Year: 1901   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  272999\n",
      "Year: 1901   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  272999\n",
      "Year: 1901   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  272999\n",
      "Year: 1901   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  272999\n",
      "Year: 1902   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  272999\n",
      "Year: 1902   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  272999\n",
      "Year: 1902   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  272999\n",
      "Year: 1902   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  272999\n",
      "Year: 1902   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  272999\n",
      "Year: 1902   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  272999\n",
      "Year: 1902   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  272999\n",
      "Year: 1902   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  272999\n",
      "Year: 1902   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  272999\n",
      "Year: 1902   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  272999\n",
      "Year: 1903   Value: -9999.0\n",
      "Set to:  0\n",
      "Cleaning at ID:  272999\n",
      "Year: 1903   Value: -9999.0\n",
      "Set to:  0\n",
      "Completed Initilization of StateID Object:  272999\n",
      "Completed Initilization of StateID Object:  273850\n",
      "Completed Initilization of StateID Object:  274399\n",
      "Completed Initilization of StateID Object:  280325\n",
      "Completed Initilization of StateID Object:  280734\n",
      "Completed Initilization of StateID Object:  280907\n",
      "Completed Initilization of StateID Object:  281582\n",
      "Completed Initilization of StateID Object:  283029\n",
      "Completed Initilization of StateID Object:  283951\n",
      "Completed Initilization of StateID Object:  284229\n",
      "Completed Initilization of StateID Object:  284987\n",
      "Completed Initilization of StateID Object:  285728\n",
      "Completed Initilization of StateID Object:  286055\n",
      "Completed Initilization of StateID Object:  287079\n",
      "Completed Initilization of StateID Object:  288816\n",
      "Completed Initilization of StateID Object:  300023\n",
      "Completed Initilization of StateID Object:  300042\n",
      "Completed Initilization of StateID Object:  300085\n",
      "Completed Initilization of StateID Object:  300093\n",
      "Completed Initilization of StateID Object:  300183\n",
      "Completed Initilization of StateID Object:  300321\n",
      "Completed Initilization of StateID Object:  300443\n",
      "Completed Initilization of StateID Object:  300687\n",
      "Completed Initilization of StateID Object:  300889\n",
      "Completed Initilization of StateID Object:  300937\n",
      "Completed Initilization of StateID Object:  301012\n",
      "Completed Initilization of StateID Object:  301185\n",
      "Completed Initilization of StateID Object:  301401\n",
      "Completed Initilization of StateID Object:  301752\n",
      "Completed Initilization of StateID Object:  301799\n",
      "Completed Initilization of StateID Object:  301966\n",
      "Completed Initilization of StateID Object:  301974\n",
      "Completed Initilization of StateID Object:  302060\n",
      "Completed Initilization of StateID Object:  302129\n",
      "Completed Initilization of StateID Object:  302610\n",
      "Completed Initilization of StateID Object:  303033\n",
      "Completed Initilization of StateID Object:  303184\n",
      "Completed Initilization of StateID Object:  303259\n",
      "Completed Initilization of StateID Object:  303319\n",
      "Completed Initilization of StateID Object:  303773\n",
      "Completed Initilization of StateID Object:  304102\n",
      "Completed Initilization of StateID Object:  304174\n",
      "Completed Initilization of StateID Object:  304555\n",
      "Completed Initilization of StateID Object:  304647\n",
      "Completed Initilization of StateID Object:  304791\n",
      "Completed Initilization of StateID Object:  304796\n",
      "Completed Initilization of StateID Object:  304844\n",
      "Completed Initilization of StateID Object:  304912\n",
      "Completed Initilization of StateID Object:  304996\n",
      "Completed Initilization of StateID Object:  305113\n",
      "Completed Initilization of StateID Object:  305426\n",
      "Completed Initilization of StateID Object:  305512\n",
      "Completed Initilization of StateID Object:  305801\n",
      "Completed Initilization of StateID Object:  306085\n",
      "Completed Initilization of StateID Object:  306164\n",
      "Completed Initilization of StateID Object:  306314\n",
      "Completed Initilization of StateID Object:  306774\n",
      "Completed Initilization of StateID Object:  306820\n",
      "Completed Initilization of StateID Object:  307167\n",
      "Completed Initilization of StateID Object:  307484\n",
      "Completed Initilization of StateID Object:  307633\n",
      "Completed Initilization of StateID Object:  308248\n",
      "Completed Initilization of StateID Object:  308383\n",
      "Completed Initilization of StateID Object:  308600\n",
      "Completed Initilization of StateID Object:  308631\n",
      "Completed Initilization of StateID Object:  308737\n",
      "Completed Initilization of StateID Object:  308906\n",
      "Completed Initilization of StateID Object:  308910\n",
      "Completed Initilization of StateID Object:  308944\n",
      "Completed Initilization of StateID Object:  309000\n",
      "Completed Initilization of StateID Object:  309292\n",
      "Completed Initilization of StateID Object:  309670\n",
      "Completed Initilization of StateID Object:  360106\n",
      "Completed Initilization of StateID Object:  361354\n",
      "Completed Initilization of StateID Object:  362537\n",
      "Completed Initilization of StateID Object:  362682\n",
      "Completed Initilization of StateID Object:  363028\n",
      "Completed Initilization of StateID Object:  363526\n",
      "Completed Initilization of StateID Object:  364385\n",
      "Completed Initilization of StateID Object:  364896\n",
      "Completed Initilization of StateID Object:  365915\n",
      "Completed Initilization of StateID Object:  366233\n",
      "Completed Initilization of StateID Object:  366689\n",
      "Completed Initilization of StateID Object:  367029\n",
      "Completed Initilization of StateID Object:  367322\n",
      "Completed Initilization of StateID Object:  367477\n",
      "Completed Initilization of StateID Object:  367931\n",
      "Completed Initilization of StateID Object:  368449\n",
      "Completed Initilization of StateID Object:  368596\n",
      "Completed Initilization of StateID Object:  368905\n",
      "Completed Initilization of StateID Object:  369050\n",
      "Completed Initilization of StateID Object:  369298\n",
      "Completed Initilization of StateID Object:  369408\n",
      "Completed Initilization of StateID Object:  369464\n",
      "Completed Initilization of StateID Object:  369728\n",
      "Completed Initilization of StateID Object:  369933\n",
      "Completed Initilization of StateID Object:  370896\n",
      "Completed Initilization of StateID Object:  374266\n",
      "Completed Initilization of StateID Object:  376698\n",
      "Completed Initilization of StateID Object:  431081\n",
      "Completed Initilization of StateID Object:  431243\n",
      "Completed Initilization of StateID Object:  431360\n",
      "Completed Initilization of StateID Object:  431580\n",
      "Completed Initilization of StateID Object:  432769\n",
      "Completed Initilization of StateID Object:  437054\n",
      "Completed Initilization of StateID Object:  437607\n",
      "Completed Initilization of StateID Object:  437612\n",
      "Completed Initilization of Objects!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse, Polygon\n",
    "\n",
    "class StateID:\n",
    "    \"\"\"\n",
    "    See above documentation for StateID class properties.\n",
    "    \"\"\"\n",
    "    winter_months = [10, 11, 0, 1]\n",
    "    spring_months = [2, 3, 4, 5]\n",
    "    summer_months = [6, 7, 8, 9]\n",
    "    \n",
    "    def __init__(self, StateID, MaxTempDict, MinTempDict, PrecipDict):\n",
    "        self._name = StateID\n",
    "        self._max_temp = MaxTempDict\n",
    "        self._min_temp = MinTempDict\n",
    "        self._precip = PrecipDict\n",
    "        \n",
    "        # these will be set later with a different file/not used for anything but filtering stations\n",
    "        self._coord = -9999 # tuple containing (latitude, longitude)\n",
    "        self._elev = -9999\n",
    "        \n",
    "        self._max_temp_winter = {}\n",
    "        self._max_temp_spring = {}\n",
    "        self._max_temp_summer = {}\n",
    "        self._min_temp_winter = {}\n",
    "        self._min_temp_spring = {}\n",
    "        self._min_temp_summer = {}\n",
    "        self._precip_winter = {}\n",
    "        self._precip_spring = {}\n",
    "        self._precip_summer = {}\n",
    "        \n",
    "        self.make_winter_max_temp_dict()\n",
    "        self.make_spring_max_temp_dict()\n",
    "        self.make_summer_max_temp_dict()\n",
    "        self.make_winter_min_temp_dict()\n",
    "        self.make_spring_min_temp_dict()\n",
    "        self.make_summer_min_temp_dict()\n",
    "        self.make_winter_precip_dict()\n",
    "        self.make_spring_precip_dict()\n",
    "        self.make_summer_precip_dict()\n",
    "    \n",
    "    #getter functions\n",
    "    def name(self):\n",
    "        return self._name\n",
    "    def max_temp(self):\n",
    "        return self._max_temp\n",
    "    def min_temp(self):\n",
    "        return self._min_temp\n",
    "    def precip(self):\n",
    "        return self._precip\n",
    "    \n",
    "    #(lat, long) and elev\n",
    "    def coord(self):\n",
    "        return self._coord\n",
    "    def elev(self):\n",
    "        return self._elev\n",
    "    \n",
    "    def max_temp_winter(self):\n",
    "        return self._max_temp_winter\n",
    "    def max_temp_spring(self):\n",
    "        return self._max_temp_spring\n",
    "    def max_temp_summer(self):\n",
    "        return self._max_temp_summer\n",
    "    def min_temp_winter(self):\n",
    "        return self._min_temp_winter\n",
    "    def min_temp_spring(self):\n",
    "        return self._min_temp_spring\n",
    "    def min_temp_summer(self):\n",
    "        return self._min_temp_summer\n",
    "    def precip_winter(self):\n",
    "        return self._precip_winter\n",
    "    def precip_spring(self):\n",
    "        return self._precip_spring\n",
    "    def precip_summer(self):\n",
    "        return self._precip_summer\n",
    "    \n",
    "    #seasonal dictionary builders\n",
    "    def make_winter_max_temp_dict(self):\n",
    "        new_dict = {}\n",
    "        for year in self._max_temp.keys():\n",
    "            new_dict[year] = self._max_temp.get(year)[0:2]\n",
    "            new_dict[year] += self._max_temp.get(year)[10:12]\n",
    "        self._max_temp_winter = new_dict\n",
    "    def make_spring_max_temp_dict(self):\n",
    "        new_dict = {}\n",
    "        for year in self._max_temp.keys():\n",
    "            new_dict[year] = self._max_temp.get(year)[2:6]\n",
    "        self._max_temp_spring = new_dict\n",
    "    def make_summer_max_temp_dict(self):\n",
    "        new_dict = {}\n",
    "        for year in self._max_temp.keys():\n",
    "            new_dict[year] = self._max_temp.get(year)[6:10]\n",
    "        self._max_temp_summer = new_dict\n",
    "    def make_winter_min_temp_dict(self):\n",
    "        new_dict = {}\n",
    "        for year in self._min_temp.keys():\n",
    "            new_dict[year] = self._min_temp.get(year)[0:2]\n",
    "            new_dict[year] += self._min_temp.get(year)[10:12]\n",
    "        self._min_temp_winter = new_dict\n",
    "    def make_spring_min_temp_dict(self):\n",
    "        new_dict = {}\n",
    "        for year in self._min_temp.keys():\n",
    "            new_dict[year] = self._min_temp.get(year)[2:6]\n",
    "        self._min_temp_spring = new_dict\n",
    "    def make_summer_min_temp_dict(self):\n",
    "        new_dict = {}\n",
    "        for year in self._min_temp.keys():\n",
    "            new_dict[year] = self._min_temp.get(year)[6:10]\n",
    "        self._min_temp_summer = new_dict\n",
    "    def make_winter_precip_dict(self):\n",
    "        new_dict = {}\n",
    "        for year in self._precip.keys():\n",
    "            new_dict[year] = self._precip.get(year)[0:2]\n",
    "            new_dict[year] += self._precip.get(year)[10:12]\n",
    "        self._precip_winter = new_dict\n",
    "    def make_spring_precip_dict(self):\n",
    "        new_dict = {}\n",
    "        for year in self._precip.keys():\n",
    "            new_dict[year] = self._precip.get(year)[2:6]\n",
    "        self._precip_spring = new_dict\n",
    "    def make_summer_precip_dict(self):\n",
    "        new_dict = {}\n",
    "        for year in self._precip.keys():\n",
    "            new_dict[year] = self._precip.get(year)[6:10]\n",
    "        self._precip_summer = new_dict\n",
    "        \n",
    "        \n",
    "print(\"checkpoint 1\")\n",
    "with open('womack.csv') as csvfile:\n",
    "    rows = csv.reader(csvfile, delimiter=',')\n",
    "    all_data = [row for row in rows]\n",
    "    \n",
    "#removing the first row to leave just data\n",
    "all_data = all_data[1:]\n",
    "#making StateID, year and month values ints, z-scores floats\n",
    "#\n",
    "#NOTE: double lowers chance of roundoff error, but float uses less memory. currently using floats\n",
    "#SEE: http://programmers.stackexchange.com/questions/188721/when-do-you-use-float-and-when-do-you-use-double\n",
    "#\n",
    "for i in all_data:\n",
    "    i[0] = int(i[0])   #StateID\n",
    "    i[1] = int(i[1])   #Year\n",
    "    i[2] = int(i[2])   #Month\n",
    "    i[3] = float(i[3]) #MaxTemp z-score\n",
    "    i[4] = float(i[4]) #MinTemp z-score\n",
    "    i[5] = float(i[5]) #Precip z-score\n",
    "\n",
    "print(\"checkpoint 2\")\n",
    "#looping over the unique StateID list to create dictionaries and initialize objects\n",
    "StateID_objects = []\n",
    "for i in range(137):\n",
    "    ID_max_temp_dict = {}\n",
    "    ID_min_temp_dict = {}\n",
    "    ID_precip_dict = {}\n",
    "    for j in range(i*1380,(i+1)*1380):\n",
    "        #we only check existence of key in the max_temp_dict because processing occurs\n",
    "        #for all of the fields at once. max_temp_dict thus is a valid proxy for keys\n",
    "        current_row = all_data[j]\n",
    "        #cleaning the bad data\n",
    "        #some stations were missing data, but no station missed more than 3 months of data\n",
    "        #from January 1908 to September 2010 (Wilkie 24)\n",
    "        #bad data entries were given values of -9999, replacing them with values of 0 (should not affect analysis)\n",
    "        for k in range(3,6):\n",
    "            if (current_row[k] == -9999.0):\n",
    "                print(\"Cleaning at ID: \", str(current_row[0]))\n",
    "                print(\"Year: \"+str(current_row[1])+ \"   Value: \"+str(current_row[k]))\n",
    "                current_row[k] = 0\n",
    "                print(\"Set to: \", current_row[k])\n",
    "        current_year = current_row[1]\n",
    "        if current_year in ID_max_temp_dict:\n",
    "            ID_max_temp_dict[current_year].append(current_row[3])\n",
    "            ID_min_temp_dict[current_year].append(current_row[4])\n",
    "            ID_precip_dict[current_year].append(current_row[5])\n",
    "        else:\n",
    "            ID_max_temp_dict[current_year] = [current_row[3]]\n",
    "            ID_min_temp_dict[current_year] = [current_row[4]]\n",
    "            ID_precip_dict[current_year] = [current_row[5]]\n",
    "    current_State_ID = all_data[i*1380][0]\n",
    "    StateID_objects.append(StateID(current_State_ID, ID_max_temp_dict, ID_min_temp_dict, ID_precip_dict))\n",
    "    print(\"Completed Initilization of StateID Object: \", current_State_ID)\n",
    "\n",
    "print(\"Completed Initilization of Objects!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation of StateID Class ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'All StateID objects are valid!'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# validating that objects contain correct indices\n",
    "# checking to make sure that every year has 12 entries for every object\n",
    "\n",
    "# this works as a valid check since we created the dictionaries using iteration, so any\n",
    "# errors would be due to incorrect indexing and would show up as issues in the number of\n",
    "# values assigned to one of the first or last years\n",
    "\n",
    "def validate_StateID(StateID_object):\n",
    "    start = StateID_object.max_temp().get(1900)\n",
    "    end = StateID_object.max_temp().get(2014)\n",
    "    if (len(start)!=12 or len(end)!=12):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def validate_all_StateID(StateID_object_list):\n",
    "    for obj in StateID_objects:\n",
    "        if validate_StateID(obj)==False:\n",
    "            return \"Error on StateID \"+str(obj.name())+\"!\"\n",
    "    return \"All StateID objects are valid!\"\n",
    "\n",
    "validate_all_StateID(StateID_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.86237941, 2.117721569, 1.608463379, -1.711390114, 0, 0, 1.305917459, -0.785074336, -1.453419333, -0.524445127, -1.519106516, -0.242608153]\n"
     ]
    }
   ],
   "source": [
    "#this is to check that the values match the csv file\n",
    "for obj in StateID_objects:\n",
    "    if obj.name() == 272999:\n",
    "        print(obj.precip().get(1903))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding coordinates and elevation to Station ID Objects ###\n",
    "Data obtained from http://www.surfacestations.org/ushcn_stationlist.htm, and stations missing from this data set were found at various sources related to https://www.ncdc.noaa.gov/data-access/land-based-station-data/land-based-datasets/us-historical-climatology-network-ushcn.\n",
    "\n",
    "Coordinates will be in a tuple, (latitude, longitude). Elevation is measured in units of feet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished adding coordinates and elevation to StateIDs!\n"
     ]
    }
   ],
   "source": [
    "with open('USHCN_ID_Collection_csv.csv') as csvfile:\n",
    "    rows = csv.reader(csvfile, delimiter=',')\n",
    "    all_data = [row for row in rows]\n",
    "\n",
    "# removing the first row to leave just data\n",
    "all_data = all_data[1:]\n",
    "for i in all_data:\n",
    "    i[0] = int(i[0])   #StateID\n",
    "    i[1] = float(i[1])   #Latitude\n",
    "    i[2] = float(i[2])   #Longitude\n",
    "    i[3] = float(i[3])   #Elevation\n",
    "\n",
    "# adding to StateID classes\n",
    "for i in StateID_objects:\n",
    "    for j in all_data:\n",
    "        if i.name() == j[0]:\n",
    "            i._coord = (j[1], j[2])\n",
    "            i._elev = j[3]\n",
    "            \n",
    "print(\"Finished adding coordinates and elevation to StateIDs!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting to West/East of Appalachian Mountains ###\n",
    "Coordinates of Appalachian mountains were chosen by using Google Maps topography map to track the eastmost \"edge\" of the mountain range. Six points were chose to approximate the extent of the mountain range. appalachian_coord takes the longitude (between -83.72 and -66.51 degrees) and returns the mountain range's corresponding latitude. We sort the StateID_objects into either West or East of the mountain range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of StateIDs West of Appalachians:  83\n",
      "Number of StateIDs East of Appalachians:  54\n"
     ]
    }
   ],
   "source": [
    "def appalachian_coord(longitude):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "            longitude (float): the longitude coordinate\n",
    "    Output:\n",
    "            Returns the latitude (float) of the Appalachian mountain range East edge corresponding to the \n",
    "            given longitude coordinate.\n",
    "    \"\"\"\n",
    "    mount_coords = [(34.50, -83.72), (37.58, -79.37), (40.00, -77.36), (41.01, -74.35), (43.80, -72.12), (47.82, -66.51)]\n",
    "    if longitude < mount_coords[1][1]:\n",
    "        b1 = mount_coords[0][0]\n",
    "        slope1 = (mount_coords[1][0]-mount_coords[0][0])/(mount_coords[1][1]-mount_coords[0][1])\n",
    "        return b1 + slope1*(longitude-mount_coords[0][1])\n",
    "    elif longitude < mount_coords[2][1]:\n",
    "        b2 = mount_coords[1][0]\n",
    "        slope2 = (mount_coords[2][0]-mount_coords[1][0])/(mount_coords[2][1]-mount_coords[1][1])\n",
    "        return b2 + slope2*(longitude-mount_coords[1][1])\n",
    "    elif longitude < mount_coords[3][1]:\n",
    "        b3 = mount_coords[2][0]\n",
    "        slope3 = (mount_coords[3][0]-mount_coords[2][0])/(mount_coords[3][1]-mount_coords[2][1])\n",
    "        return b3 + slope3*(longitude-mount_coords[2][1])\n",
    "    elif longitude < mount_coords[4][1]:\n",
    "        b4 = mount_coords[3][0]\n",
    "        slope4 = (mount_coords[4][0]-mount_coords[3][0])/(mount_coords[4][1]-mount_coords[3][1])\n",
    "        return b4 + slope4*(longitude-mount_coords[3][1])\n",
    "    else:\n",
    "        b5 = mount_coords[4][0]\n",
    "        slope5 = (mount_coords[5][0]-mount_coords[4][0])/(mount_coords[5][1]-mount_coords[4][1])\n",
    "        return b5 + slope5*(longitude-mount_coords[4][1])\n",
    "    \n",
    "def app_west_east(State_ID_object):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "            State_ID_object (StateID_object): the StateID object to consider\n",
    "    Output:\n",
    "            True if the State_ID_object is located West of the Appalachian mountain range,\n",
    "            False if the State_ID_object is East of the mountain range.\n",
    "    \"\"\"\n",
    "    ID_coords = State_ID_object.coord()\n",
    "    app_lat = appalachian_coord(ID_coords[1])\n",
    "    if app_lat < ID_coords[0]:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def app_sorter(ID_obj_list):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "            ID_obj_list (list of StateID_objects): the list of StateID objects to sort\n",
    "    Output:\n",
    "            A 2D list, first list is the StateID objects West of the Appalachian mountain range,\n",
    "            second list is the StateID objects East of the mountain range.\n",
    "    \"\"\"\n",
    "    west_list = []\n",
    "    east_list = []\n",
    "    for i in ID_obj_list:\n",
    "        if app_west_east(i) == True:\n",
    "            west_list.append(i)\n",
    "        else:\n",
    "            east_list.append(i)\n",
    "    return [west_list, east_list]\n",
    "\n",
    "Appalachian_StateID_objects = app_sorter(StateID_objects)\n",
    "print(\"Number of StateIDs West of Appalachians: \", len(Appalachian_StateID_objects[0]))\n",
    "print(\"Number of StateIDs East of Appalachians: \", len(Appalachian_StateID_objects[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing and Displaying Data\n",
    "------------------------------\n",
    "1. The select_id function returns the object corresponding to the given ID.\n",
    "2. The dict_to_list function takes the data dictionaries from the StateID classes and converts them to a list, with the values in chronological order.\n",
    "3. The gaussian function is simply a function that gives the value of the normalized gaussian centered at mu for a specified x value and standard deviation. This is used when we want to use gaussian kernel smoothing.\n",
    "4. The normal_kernel_constructor creates an array of uniformly sampled values of the gaussian function to smooth the data with.\n",
    "5. The gaussian_smoother function takes an array of x-axis data, y-axis data, and a standard deviation, and smooths the y-axis data with a gaussian kernel according to the given standard deviation. It outputs a 2-D array containing the correct truncated x-axis data and smoothed y-axis data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def select_id(id_number, id_obj_list):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "            id_number (int): the ID number of the desired StateID.\n",
    "            id_obj_list (list): the list of StateID objects created in the first part of the code.\n",
    "    Output:\n",
    "            the StateID object with name corresponding to the given id_number.\n",
    "    \"\"\"\n",
    "    for obj in id_obj_list:\n",
    "        if obj.name() == id_number:\n",
    "            return obj\n",
    "    \n",
    "def dict_to_list(dictionary):\n",
    "    converted_list = []\n",
    "    #FOR SOME REASON 3.5 HAS THE KEYS IN A \"VIEW\" OBJECT, convert to list\n",
    "    keys = list(dictionary.keys())\n",
    "    keys.sort()\n",
    "    for i in keys:\n",
    "        converted_list += dictionary.get(i)\n",
    "    return converted_list\n",
    "\n",
    "def gaussian(x, mu, sig):\n",
    "    return 1./(np.sqrt(2.*np.pi)*sig)*np.exp(-np.power((x - mu)/sig, 2.)/2)\n",
    "\n",
    "def normal_kernel_constructor(std_dev):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "            std_dev (float): the standard deviation of the kernel to be used, unit of years\n",
    "    NOTE:   std_dev must give an int when multiplied by 12 so that we get an integer\n",
    "            number of sampled points for the kernel!\n",
    "    Output:\n",
    "            list of discretely sampled kernel values corresponding to the normal\n",
    "            distribution, covering 95% of kernel (2 sigma on either side of zero)\n",
    "    \"\"\"\n",
    "    width = 2*std_dev #one sided width\n",
    "    n_points = 2*width*12+1\n",
    "    bins = n_points-1\n",
    "    sampled_points = np.linspace(-width, width, num=n_points, endpoint=True)\n",
    "    sampled_values = []\n",
    "    for i in sampled_points:\n",
    "        x = gaussian(i, 0, std_dev)*2*width/bins\n",
    "        sampled_values.append(x)\n",
    "    total = sum(sampled_values)\n",
    "    renormalize_offset = (1-total)/n_points\n",
    "    for i in range(len(sampled_values)):\n",
    "        sampled_values[i] += renormalize_offset\n",
    "    return sampled_values\n",
    "\n",
    "def gaussian_smoother(x_array, y_array, std_dev):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "            x_values (array_like): an array of points for the x-axis\n",
    "            y_values (array_like): an array of the data to be smoothed\n",
    "            std_dev (float):  the std_dev of the kernel to be used, unit of years. Refer to normal_kernel_constructor.\n",
    "    Output:\n",
    "            returns a 2-D numpy array. 1st array contains the x-values corresponding to the smoothed\n",
    "            y-values. 2nd array contains the smoothed y-data.\n",
    "    NOTE: Assumes that y_array, x_array are same dimension, and that the std_dev given does not make\n",
    "          the gaussian kernel larger than the dimension of the y_array!\n",
    "    \"\"\"\n",
    "    #NOTE: x-array is truncated at both endpoints since the gaussian window starts at the first\n",
    "    #point where it does not extend past the data.\n",
    "    n_of_width_points = int(2*std_dev*12) #number of points required on each side of mean for kernel\n",
    "    x_array_dimension = len(x_array)\n",
    "    truncated_x_list = list(x_array[n_of_width_points:x_array_dimension-n_of_width_points])\n",
    "    \n",
    "    kernel_values = normal_kernel_constructor(std_dev)\n",
    "    smoothed_y_list = []\n",
    "    for i in range(n_of_width_points, x_array_dimension-n_of_width_points):\n",
    "        smooth_pass = 0\n",
    "        for j in range(len(kernel_values)):\n",
    "            smooth_pass += y_array[i+j-n_of_width_points]*kernel_values[j]\n",
    "        smoothed_y_list.append(smooth_pass)\n",
    "        \n",
    "    return [truncated_x_list, smoothed_y_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Index and Averaging Methods ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nei_index(StateID_obj):\n",
    "    \"\"\"\n",
    "    Returns an list containing the monthly NEI index values for the given StateID obj.\n",
    "    Inputs:\n",
    "            StateID_obj: a StateID object\n",
    "    Outputs:\n",
    "            Returns a 1-D list for the monthly NEI index for the given object.\n",
    "    \"\"\"\n",
    "    max_temp_list = dict_to_list(StateID_obj.max_temp())\n",
    "    min_temp_list = dict_to_list(StateID_obj.min_temp())\n",
    "    precip_list = dict_to_list(StateID_obj.precip())\n",
    "    nei_index_list = []\n",
    "    for i in range(len(max_temp_list)):\n",
    "        nei_value = (max_temp_list[i]+min_temp_list[i]+precip_list[i])/3\n",
    "        nei_index_list.append(nei_value)\n",
    "    return nei_index_list\n",
    "    \n",
    "def average_StateID_series(StateID_obj_list):\n",
    "    \"\"\"\n",
    "    Returns a 4-D list containing the averaged max_temp, min_temp, precip and indexed values for each month\n",
    "    across all of the objects in the list.\n",
    "    Inputs:\n",
    "            StateID_obj_list (list): list of StateID objects to average.\n",
    "    Outputs:\n",
    "            Returns a 4-D list. \n",
    "                average_StateID_series()[0] -> averaged max_temp list\n",
    "                average_StateID_series()[1] -> averaged min_temp list\n",
    "                average_StateID_series()[2] -> averaged precip list\n",
    "                average_StateID_series()[3] -> averaged NEI index list\n",
    "    \"\"\"\n",
    "    N = float(len(StateID_obj_list))\n",
    "    max_temp_lists = []\n",
    "    min_temp_lists = []\n",
    "    precip_lists = []\n",
    "    nei_index_lists = []\n",
    "    for obj in StateID_obj_list:\n",
    "        max_temp = dict_to_list(obj.max_temp())\n",
    "        min_temp = dict_to_list(obj.min_temp())\n",
    "        precip = dict_to_list(obj.precip())\n",
    "        nei = nei_index(obj)\n",
    "        max_temp_lists.append(max_temp)\n",
    "        min_temp_lists.append(min_temp)\n",
    "        precip_lists.append(precip)\n",
    "        nei_index_lists.append(nei)\n",
    "    avg_max_temp_list = list(np.divide(np.sum(max_temp_lists, axis=0), N))\n",
    "    avg_min_temp_list = list(np.divide(np.sum(min_temp_lists, axis=0), N))\n",
    "    avg_precip_list = list(np.divide(np.sum(precip_lists, axis=0), N))\n",
    "    avg_nei_list = list(np.divide(np.sum(nei_index_lists, axis=0), N))\n",
    "    return [avg_max_temp_list, avg_min_temp_list, avg_precip_list, avg_nei_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating Noisy Data\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def red_noise_generator(r, std_dev, length):\n",
    "    \"\"\"\n",
    "    Using algorithm described by: http://www.atmos.washington.edu/~breth/classes/AM582/lect/lect8-notes.pdf\n",
    "    NOTE: Additive red noise?\n",
    "    \n",
    "    Inputs:\n",
    "            r (float): the lag-1 autocorrelation coefficient\n",
    "            std_dev (float): the standard deviation for gaussian white noise used to generate the red noise sequence\n",
    "            length (int): length of the sequence to be generated\n",
    "            \n",
    "    Output:\n",
    "            list of floats for the red noise sequence\n",
    "    \"\"\"\n",
    "    white_noise_sequence = np.random.normal(0, std_dev, length)\n",
    "    white_noise_sequence = white_noise_sequence.tolist()\n",
    "    red_noise_sequence = [white_noise_sequence[0]]\n",
    "    for i in range(1, len(white_noise_sequence)):\n",
    "        next_red_noise = r*red_noise_sequence[i-1] + np.sqrt(1-r*r)*white_noise_sequence[i]\n",
    "        red_noise_sequence.append(next_red_noise)\n",
    "    return red_noise_sequence\n",
    "\n",
    "def line_generator(y_int, slope, x_list):\n",
    "    \"\"\"\n",
    "    Generates a list of floats of a given length for a line. Used to make noisy data.\n",
    "    Inputs:\n",
    "            y_int (float): the y intercept of the line\n",
    "            slope (float): the slope of the line\n",
    "            x_list (list of floats): x values for the line. list is assumed to start at zero and be in ascending order.\n",
    "    \n",
    "    Output:\n",
    "            list of y values (floats) corresponding to the desired line\n",
    "    \"\"\"\n",
    "    y_list = [y_int]\n",
    "    for i in range(len(x_list)-1):\n",
    "        dy = (x_list[i+1]-x_list[i])*slope\n",
    "        y_list.append(y_list[i]+dy)\n",
    "    return y_list\n",
    "\n",
    "def noisy_line_generator(x_list, y_list, noise_list):\n",
    "    \"\"\"\n",
    "    Combines the given noise list with the given y_list to generate linear, noisy data.\n",
    "    Inputs:\n",
    "            x_list (list): the x values for the line, assumed to start at zero and be in ascending order\n",
    "            y_list (list): the y values for the line\n",
    "            noise_list (list): the noisy values to be combined with the linear data\n",
    "            NOTE: All lists must be same length!\n",
    "    \n",
    "    Outputs:\n",
    "            2-D list with first dimension containing the x values, second dimension containing the noisy y values\n",
    "    \"\"\"\n",
    "    new_data = []\n",
    "    for i in range(len(y_list)):\n",
    "        new_data.append(y_list[i]+noise_list[i])\n",
    "    return [x_list, new_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wavelets\n",
    "--------\n",
    "Each different wavelet is its own class that encapsulates the following methods:\n",
    "1. Scaling\n",
    "2. Sampling\n",
    "3. Renormalization\n",
    "\n",
    "Each wavelet takes a float, defining the scale parameter of the wavelet, and calculates the number of sampled points allowed by this scale. Note that the number of sampled points will be odd, so that the central value has an equal number of points on either side for convolution. The wavelet is then uniformly sampled depending on the number of allowed sampling points. Since sampling provides only an approximation of the wavelet, we renormalize the sampled wavelet points so that the wavelet power is equal to one. This is done by numerically integrating the squared sampled wavelet points, subtracting the result from 1, averaging the difference over the number of points, and subtracting the averaged difference from each point. This is similar to renormalizing the sampled values of a gaussian PDF to preserve signal amplitude for smoothing purposes.\n",
    "\n",
    "Implemented wavelets:\n",
    "1. Morlet (complex)\n",
    "2. Harr (real)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Morlet Wavelet ####\n",
    "Defined in notes. Also have derivations of scaling methods and normalizing coefficients in notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Morlet_Wavelet:  \n",
    "    def __init__(self, scale, f):\n",
    "        self._scale = scale\n",
    "        self._f = f\n",
    "    def f(self):\n",
    "        return self._f\n",
    "    def scale(self):\n",
    "        return self._scale\n",
    "    \n",
    "    def morlet_function(self):\n",
    "        \"\"\"\n",
    "        Calculates the values of the Morlet wavelet, centered at zero, w/ frequency f.\n",
    "        Outputs:\n",
    "                Returns the value of Morlet wavelet for each input value, list.\n",
    "                NOTE: These values are complex numbers.\n",
    "        \"\"\"\n",
    "        x = np.linspace(-2, 2, 2*self.sampling_points()+1)\n",
    "        norm_coeff = np.power(np.pi, -1/4)\n",
    "        complex_array = [-np.complex(0,1)*self.f()*i for i in x]\n",
    "        neg_square_list = [-i*i/2 for i in x]\n",
    "        sampled_values = [norm_coeff*np.exp(complex_array[i])*np.exp(neg_square_list[i]) for i in range(len(complex_array))]\n",
    "        renorm_sampled_values = self.renormalize(sampled_values)\n",
    "        final_sampled_values = [(1/np.sqrt(self._scale))*i for i in renorm_sampled_values]\n",
    "        return final_sampled_values\n",
    "    \n",
    "    def scale_to_year(self):\n",
    "        \"\"\"\n",
    "        Calculates the scale to year conversion for a given frequency and scale.\n",
    "        Outputs:\n",
    "                Returns the number of years the scale represents, float.\n",
    "        \"\"\"\n",
    "        scale_factor = 4*np.pi/(self.f()+np.sqrt(2+self.f()*self.f()))\n",
    "        return scale_factor*self.scale()\n",
    "    \n",
    "    def sampling_points(self):\n",
    "        \"\"\"\n",
    "        Calculates the number of points to sample the wavelet with on one side (tail).\n",
    "        Outputs:\n",
    "                Returns the number of one sided sampling points, int.\n",
    "        \"\"\"\n",
    "        years_for_scale = self.scale_to_year()\n",
    "        one_sided_width = 2*years_for_scale # 2*scale corresponds to 95% of gaussian envelope [see notes]\n",
    "        one_sided_months = 12*one_sided_width\n",
    "        one_sided_points = np.ceil(one_sided_months)\n",
    "        return one_sided_points\n",
    "        \n",
    "    def renormalize(self, sampled_values):\n",
    "        \"\"\"\n",
    "        Renormalization helper function for morlet_function.\n",
    "        Sets total power of sampled wavelet coefficients to 1.\n",
    "        \"\"\"\n",
    "        # NOTE: It turns out that even for small scales the wavelet power is very close to 1\n",
    "        # -> renormalization may not be necessary\n",
    "        power = 0\n",
    "        renorm_values = []\n",
    "        d_power = 4/(2*self.sampling_points())\n",
    "        for i in sampled_values:\n",
    "            magnitude = np.absolute(i)\n",
    "            power += magnitude*magnitude*d_power\n",
    "        \n",
    "        power_scale_factor = np.sqrt(1/power)\n",
    "        \n",
    "        for i in sampled_values:\n",
    "            renorm_values.append(i*power_scale_factor)\n",
    "        return renorm_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Harr Wavelet ####\n",
    "Scaling method and normalization are trivial. Scale directly corresponds to the year width of the wavelet. Notes on the Harr wavelet are found for 10/15.\n",
    "\n",
    "The Harr wavelet corresponds to a difference in averages. The wavelet coefficient is the difference in the average of the first half of the data within the support of the wavelet, and the average of the second half of the data within the support. A positive value corresponds to the first half having a higher average, while a negative value corresponds to the second half having a higher average.\n",
    "\n",
    "NOTE: I decided to implement the Harr wavelet so that the 1st value of the wavelet corresponds to the current value in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Harr_Wavelet:\n",
    "    \"\"\"\n",
    "    NOTE: SCALE MUST BE A MULTIPLE OF 1/6.\n",
    "    \"\"\"\n",
    "    def __init__(self, scale):\n",
    "        self._scale = scale\n",
    "    def scale(self):\n",
    "        return self._scale\n",
    "    \n",
    "    def harr_function(self):\n",
    "        \"\"\"\n",
    "        Returns a list corresponding to the values of the Harr wavelet for each input value.\n",
    "        These values are already normalized given the scale. Thus, each Harr wavelet class\n",
    "        object corresponds to a daughter wavelet.\n",
    "        Outputs:\n",
    "                Returns the Harr wavelet values at each index, list.\n",
    "        \"\"\"\n",
    "        scale = self.scale()\n",
    "        normalized_value = 1/np.sqrt(12*scale)\n",
    "        first_avg = [normalized_value]*(scale*6)\n",
    "        second_avg = [-1*normalized_value]*(scale*6)\n",
    "        harr_values = first_avg + second_avg\n",
    "        return harr_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolution Method\n",
    "------------------\n",
    "Takes a time series array and a wavelet array, returns the wavelet coefficients for the continuous wavelet transform.\n",
    "\n",
    "Pads the array with zeros depending on COI of given wavelet array. Returned wavelet coefficient array entries correspond to the times in the original time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# these methods are for standardizing the Morlet wavelet scales\n",
    "def scale_range(scale_min, scale_max):\n",
    "    \"\"\"\n",
    "    Standardizes the granularity of scale ranges.\n",
    "    \"\"\"\n",
    "    return list(np.arange(scale_min, scale_max+0.25, 0.25)) \n",
    "\n",
    "def scale_range_years(scale_list, morlet_freq):\n",
    "    \"\"\"\n",
    "    Returns a list of the corresponding years in each scale for the Morlet wavelet with given frequency.\n",
    "    \"\"\" \n",
    "    year_list = []\n",
    "    scale_factor = 4*np.pi/(morlet_freq+np.sqrt(2+morlet_freq*morlet_freq))\n",
    "    for i in scale_list:\n",
    "        year_list.append(i*scale_factor)\n",
    "    return year_list\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------------- #\n",
    "# these methods are only for the Morlet wavelet, different continuous convolution method for Harr wavelet\n",
    "def continuous_convolution(time_series, wavelet_list):\n",
    "    # this is convolving one Morlet wavelet with the time series\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "            time_series (array_like): time series values to be transformed\n",
    "            wavelet_array (array_like): array of sampled, renormalized values from wavelet\n",
    "            NOTE: wavelet_array must have an odd number of values! (for Morlet wavelet, may need different method\n",
    "            for the Harr wavelet)\n",
    "    Outputs:\n",
    "            Returns an array with the Morlet wavelet coefficients.\n",
    "    \"\"\"\n",
    "    coeffs_list = []\n",
    "    n = len(time_series)\n",
    "    m = len(wavelet_list)\n",
    "    pad_len = int((m-1)/2) #one sided tail length\n",
    "    padding_list = [0]*pad_len\n",
    "    pad_time_series = padding_list + time_series + padding_list\n",
    "    for i in range(pad_len, pad_len + n):\n",
    "        coeff = 0\n",
    "        for j in range(i-pad_len, i+pad_len+1):\n",
    "            coeff += pad_time_series[j]*wavelet_list[j+pad_len-i]\n",
    "        coeffs_list.append(coeff)\n",
    "    return coeffs_list\n",
    "\n",
    "def continuous_transform_morlet(time_series, scale_array, f):\n",
    "    # this is convolving a set of Morlet wavelets with the time series\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "            time_series (array_like): time series values to be transformed\n",
    "            scale_array (array_like): array of values for wavelet scales\n",
    "            f (float): frequency of Morlet wavelet\n",
    "    Outputs:\n",
    "            2D array. Rows are wavelet coefficients for a given scale, columns are wavelet coefficient at each\n",
    "            time series position.\n",
    "    \"\"\"\n",
    "    wavelet_list = []\n",
    "    wavelet_coefficients = []\n",
    "    for i in scale_array:\n",
    "        wvlt = Morlet_Wavelet(i, f)\n",
    "        wavelet_list.append(wvlt.morlet_function())    \n",
    "    for i in wavelet_list:\n",
    "        wavelet_coefficients.append(continuous_convolution(time_series, i))\n",
    "    return wavelet_coefficients\n",
    "\n",
    "\n",
    "\n",
    "# these methods are for standardizing the granularity of the Harr wavelet scales\n",
    "def harr_scale_range(scale_min, scale_max):\n",
    "    \"\"\"\n",
    "    Standardizes the granularity of Harr scale ranges.\n",
    "    Currently set so that each next scale corresponds to one more month of data added to each average.\n",
    "    I.e. two data points are added onto the width of the next wavelet.\n",
    "    \"\"\"\n",
    "    granularity = 1.0/6.0\n",
    "    return list(np.arange(scale_min, scale_max+granularity, granularity))\n",
    "# don't need to convert to years as scale = year for Harr wavelet\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------------- #\n",
    "# these methods are for convolution of the Harr wavelet\n",
    "def harr_convolution(time_series, wavelet_list):\n",
    "    # this is convolving one Harr wavelet with the time series\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "            time_series (array_like): time series values to be transformed\n",
    "            wavelet_array (array_like): array of Harr wavelet values\n",
    "            NOTE: wavelet_array must have an even number of values!\n",
    "    Outputs:\n",
    "            Returns an array with the Harr wavelet coefficients\n",
    "    \"\"\"\n",
    "    coeffs_list = []\n",
    "    n = len(time_series)\n",
    "    m = len(wavelet_list)\n",
    "    pad_len = m #we are only padding the end of the time series due to the Harr wavelet starting at current point\n",
    "    padding_list = [0]*pad_len\n",
    "    pad_time_series = time_series + padding_list\n",
    "    for i in range(0, n):\n",
    "        coeff = 0\n",
    "        for j in range(i, i+m):\n",
    "            coeff += pad_time_series[j]*wavelet_list[j-i]\n",
    "        coeffs_list.append(coeff)\n",
    "    return coeffs_list\n",
    "\n",
    "def continuous_transform_harr(time_series, scale_array):\n",
    "    # this is convolving a set of Harr wavelets with the time series\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "            time_series (array_like): time series values to be transformed\n",
    "            scale_array (array_like): list of values for Harr wavelet scales\n",
    "    Outputs:\n",
    "            ND list. Rows are wavelet coefficients for each given scale, columns are wavelet coefficient at each\n",
    "            time series position.\n",
    "    \"\"\"\n",
    "    wavelet_list = []\n",
    "    wavelet_coefficients = []\n",
    "    for i in scale_array:\n",
    "        wvlt = Harr_Wavelet(i)\n",
    "        wavelet_list.append(wvlt.harr_function())    \n",
    "    for i in wavelet_list:\n",
    "        wavelet_coefficients.append(harr_convolution(time_series, i))\n",
    "    return wavelet_coefficients\n",
    "    \n",
    " \n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------------- #\n",
    "# these methods are for computing the power of the wavelet coefficients\n",
    "def complex_list_square(old_list):\n",
    "    mag_list = np.absolute(old_list)\n",
    "    return list(np.square(mag_list))\n",
    "\n",
    "def wavelet_power_spectra(wavelet_coefficients):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "            wavelet_coefficients (2D List): 2D list of complex wavelet coefficients. Rows are coefficients for each scale\n",
    "    Output:\n",
    "            2D list of the power of each wavelet coefficient. Rows are squared magnitudes at each scale\n",
    "    \"\"\"\n",
    "    power_spectra_list = []\n",
    "    for i in wavelet_coefficients:\n",
    "        power_spectra_list.append(complex_list_square(i))\n",
    "    return power_spectra_list\n",
    "\n",
    "def normalized_wavelet_power_spectra(wavelet_spectra, variance):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "            wavelet_spectra (2D List): the power of each wavelet coefficient. Rows are squared magnitudes at each scale\n",
    "    Output:\n",
    "            2D list of the normalized power of each wavelet coefficient. Rows are squared normalized magnitude at each scale\n",
    "    \"\"\"\n",
    "    normalized_spectra = []\n",
    "    for i in range(len(wavelet_spectra)):\n",
    "        normalized_spectra_row = []\n",
    "        for j in range(len(wavelet_spectra[0])):\n",
    "            normalized_spectra_row.append((wavelet_spectra[i][j])/variance)\n",
    "        normalized_spectra.append(normalized_spectra_row)\n",
    "    return normalized_spectra\n",
    "\n",
    "def wavelet_power_smoothing(x_data, wavelet_power_list, std_dev):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "            x_data (list): the time coordinates of the data\n",
    "            wavelet_power_list (list): the 2D list of wavelet power coefficients, rows correspond to the scale\n",
    "            std_dev (float): the standard deviation of the gaussian kernel to smooth with, units of years\n",
    "    Output:\n",
    "            2D list, 1st entry is truncated x_data, 2nd entry is 2D list corresponding to the smoothed wavelet power\n",
    "            spectra at each scale\n",
    "    \"\"\"\n",
    "    smoothed_wavelet_power = []\n",
    "    for scale_row in wavelet_power_list:\n",
    "        new_data = gaussian_smoother(x_data, scale_row, std_dev)\n",
    "        smoothed_wavelet_power.append(new_data[1])\n",
    "    return [new_data[0], smoothed_wavelet_power]\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------------- #\n",
    "# these methods are for adding the COI to a plot\n",
    "def coi_left(x_data_start, period_range):\n",
    "    \"\"\"\n",
    "    Calculates the COI line for the left side of the COI of the Morlet wavelet.\n",
    "    Inputs:\n",
    "            x_data_start (float): the starting time coordinate of the time series\n",
    "            period_range (list): the periods of the wavelets, in units of years\n",
    "    Outputs:\n",
    "            List of 2D coordinates for the polygon that corresponds to the COI\n",
    "    \"\"\"\n",
    "    coi_x_data = [x_data_start]\n",
    "    coi_x_data.append(2*period_range[0]+x_data_start)\n",
    "    coi_x_data.append(2*period_range[-1]+x_data_start)\n",
    "    coi_x_data.append(x_data_start)\n",
    "    coi_y_data = [period_range[0], period_range[0], period_range[-1], period_range[-1]]\n",
    "    coords = zip(coi_x_data, coi_y_data)\n",
    "    coords_list = []\n",
    "    for i in coords:\n",
    "        coords_list.append(list(i))\n",
    "    return coords_list\n",
    "\n",
    "def coi_right(x_data_end, period_range):\n",
    "    \"\"\"\n",
    "    Calculates the COI line for the left side of the COI of the Morlet wavelet.\n",
    "    Inputs:\n",
    "            x_data_start (float): the starting time coordinate of the time series\n",
    "            period_range (list): the periods of the wavelets, in units of years\n",
    "    Outputs:\n",
    "            List of 2D coordinates for the polygon that corresponds to the COI\n",
    "    \"\"\"\n",
    "    coi_x_data = [x_data_end, x_data_end, x_data_end-(2*period_range[-1]), x_data_end-(2*period_range[0])]\n",
    "    coi_y_data = [period_range[0], period_range[-1], period_range[-1], period_range[0]]\n",
    "    coords = zip(coi_x_data, coi_y_data)\n",
    "    coords_list = []\n",
    "    for i in coords:\n",
    "        coords_list.append(list(i))\n",
    "    return coords_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Testing ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: This statistical testing algorithm has a problem where the amplitude of the wavelet coefficients is about an order of magnitude larger than expected. Do not know if this is due to the wavelet implementation, but currently the statistical testing algorithm implemented in Torrence and Compo does not work here.\n",
    "\n",
    "\n",
    "UPDATE 3/12/17: The statistical testing fails not because the methods are written incorrectly, but because the data does not match the assumptions made by Torrence and Compo. In particular, they assume that the original data is well approximated by a red-noise lag-1 autocorrelated random process. In this case, because every value is the z-score of the original data taken with the mean of the entire data set, every value is highly correlated to every other value. This means that the data will not have a power spectrum close to that of red noise, and the statistical testing method fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# taken from: http://passel.unl.edu/pages/informationmodule.php?idinformationmodule=1130447119&topicorder=8&maxto=16&minto=1\n",
    "chi_squared_two_95 = 5.99\n",
    "\n",
    "def autocorrelation(n, time_series):\n",
    "    \"\"\"\n",
    "    Calculates the autocorrelation at lag n. Assumes time_series length is long enough to use approximation of\n",
    "    autocorrelation.\n",
    "    Inputs:\n",
    "            n (int): the lag value for the autocorrelation coefficient to be computed at\n",
    "            time_series (list): the time series data\n",
    "    Outputs:\n",
    "            Returns the autocorrelation coefficient at lag n, float.\n",
    "    \"\"\"\n",
    "    mean = sum(time_series)/float(len(time_series))\n",
    "    numerator = 0\n",
    "    denominator = 0\n",
    "    for i in range(len(time_series)-n):\n",
    "        numerator += (time_series[i]-mean)*(time_series[i+n]-mean)\n",
    "    for i in range(len(time_series)):\n",
    "        denominator += (time_series[i]-mean)*(time_series[i]-mean)\n",
    "    return numerator/denominator\n",
    "\n",
    "def sample_variance(time_series):\n",
    "    \"\"\"\n",
    "    Calculates the unbiased sample variance of the time series.\n",
    "    Inputs:\n",
    "            time_series (list): the time series that we wish to find the variance of\n",
    "    Outputs:\n",
    "            Returns the unbiased sample variance of the time series, float.\n",
    "    \"\"\"\n",
    "    mean = sum(time_series)/len(time_series)\n",
    "    var_sum = 0\n",
    "    for i in range(len(time_series)):\n",
    "        var_sum += (time_series[i]-mean)*(time_series[i]-mean)\n",
    "    return var_sum/(len(time_series)-1)\n",
    "\n",
    "def theory_wavelet_spectra(alpha, period, N):\n",
    "    \"\"\"\n",
    "    Calculates the theoretical normalized wavelet power for a wavelet of given Fourier period, assuming a red noise\n",
    "    process of length N and lag-1 autocorrelation alpha.\n",
    "    Inputs:\n",
    "            alpha (float): the assumed lag-1 autocorrelation value of the time series\n",
    "            period (float): the Fourier period of the wavelet of interest\n",
    "                            NOTE: PERIOD UNIT MUST BE IN MONTHS!\n",
    "            N (integer): the number of values in the time series\n",
    "    Outputs:\n",
    "            Returns the theoretical normalized wavelet power.\n",
    "    \"\"\"\n",
    "    k = N/period\n",
    "    P = (1-alpha*alpha)/(1+alpha*alpha-2*alpha*np.cos(2*np.pi*k/N))\n",
    "    return P\n",
    "\n",
    "def theory_95_confidence_spectra(alpha, period, N):\n",
    "    \"\"\"\n",
    "    Same as theory_wavelet_spectra, but returns the theoretical 95% confidence wavelet power threshold.\n",
    "    NOTE: For complex wavelet only, since complex wavelets have 2 DOF.\n",
    "    \"\"\"\n",
    "    k = N/period\n",
    "    P = (1-alpha*alpha)/(1+alpha*alpha-2*alpha*np.cos(2*np.pi*k/N))\n",
    "    return (1/2)*P*chi_squared_two_95\n",
    "\n",
    "def base_power_subtraction(wavelet_power_spectrum, year_range, alpha):\n",
    "    \"\"\"\n",
    "    Removes the theoretical 95% confidence spectra from the wavelet power spectra. Anything below 0 is set to 0.\n",
    "    The non-zero data thus represents the statistically significant peaks in the data.\n",
    "    Inputs:\n",
    "            wavelet_power_spectrum (2D list): the data containing the wavelet power for the time series. Rows should\n",
    "                                              correspond to different scales\n",
    "            year_range (list): the year values for each scale, used to calculate the theoretical spectra at each scale\n",
    "            alpha (float): the lag-1 autocorrelation of the time series\n",
    "    Outputs:\n",
    "            2D List containing the theoretical 95% confidence spectra subtracted from the wavelet power spectra.\n",
    "    NOTE: The year_range must match the scale range of wavelet_power_spectra!\n",
    "    \"\"\"\n",
    "    subtracted_spectra = []\n",
    "    N = len(wavelet_power_spectrum[0])\n",
    "    theoretical_spectra = []\n",
    "    for i in year_range:\n",
    "        theoretical_spectra.append(theory_95_confidence_spectra(alpha, i*12, N))\n",
    "    for i in range(len(wavelet_power_spectrum)):\n",
    "        subtracted_year = []\n",
    "        for j in range(len(wavelet_power_spectrum[0])):\n",
    "            new_val = wavelet_power_spectrum[i][j]-theoretical_spectra[i]\n",
    "            if (new_val < 0):\n",
    "                new_val = 0\n",
    "            subtracted_year.append(new_val)\n",
    "        subtracted_spectra.append(subtracted_year)\n",
    "    return subtracted_spectra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Batches of Random Data #\n",
    "Creating batches of 100 sets of randomly generated time series to calculate approximate expected power distribution of wavelet coefficients for the raw data. The data will then be z-score transformed and the wavelet coefficients will be calculated for every set in the batch, and the average wavelet coefficients of the z-scored data will be compared to the average wavelet coefficients of the raw data to determine if there is a correlation between the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def no_baseline_red_noise_batch(r, std_dev, length, batch_size):\n",
    "    \"\"\"\n",
    "    Creates a batch of red noise data with given length, standard deviation for Gaussian noise, and lag-1 autocorrelation r,\n",
    "    that has no added baseline. \n",
    "    Inputs: \n",
    "        r (float): lag-1 autocorrelation for the red noise generator\n",
    "        std_dev (float): standard deviation for Gaussian white-noise\n",
    "        length (int): number of data points to generate\n",
    "        batch_size (int): number of sets of data to generate\n",
    "    Outputs:\n",
    "        N-dim list where each list corresponds to one generated red noise data set of the given length.\n",
    "    \"\"\"\n",
    "    batch_list = []\n",
    "    for i in range(0, batch_size):\n",
    "        batch_list.append(red_noise_generator(r, std_dev, length))\n",
    "    return batch_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming Batches of Random Data #\n",
    "Applies transform to each set of the given batch. Separate function z-scores the sets in the batch, then applies transform. All functions return the AVERAGE wavelet coefficients for the whole batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_wavelet_transform(batch_lists):\n",
    "    \"\"\"\n",
    "    Takes a list of lists of raw data, returns the average wavelet coefficients for the data set.\n",
    "    Inputs:\n",
    "        batch_lists (list of lists): lists of raw data to be transformed\n",
    "        NOTE: ASSUMES MORE THAN ONE LIST!\n",
    "        NOTE: Assumes that each list is the same length.\n",
    "    Outputs:\n",
    "        List containing average wavelet coefficients.\n",
    "    \"\"\"\n",
    "    wavelet_coeff_unavg = []\n",
    "    batch_size = len(batch_lists)\n",
    "    N = len(batch_lists[0])\n",
    "    wavelet_scales = scale_range(0.5, 30)\n",
    "    M = len(wavelet_scales)\n",
    "    for i in range(0, batch_size):\n",
    "        wavelet_coeff_unavg.append(continuous_transform_morlet(batch_lists[i], wavelet_scales, 2*np.pi))\n",
    "\n",
    "    # now we average the coefficients. wavelet_coefficients_unaveraged is a list of 2-dim lists containing the wavelet\n",
    "    # coefficients. \n",
    "    # creating a list of lists of zero with same dimension as the wavelet coefficients list of lists\n",
    "    wavelet_coeff_avg = []\n",
    "    zero_list = [0]*N\n",
    "    for i in range(0, M):\n",
    "        wavelet_coeff_avg.append(zero_list)\n",
    "    # summing all wavelet values from each data set, averaging by the number of data sets\n",
    "    for i in range(0, batch_size):\n",
    "        for j in range(0, M):\n",
    "            wavelet_coeff_avg[j] = [sum(x) for x in zip(wavelet_coeff_avg[j], wavelet_coeff_unavg[i][j])]\n",
    "    for j in range(0, M):\n",
    "        for k in range(0, N):\n",
    "            wavelet_coeff_avg[j][k] = wavelet_coeff_avg[j][k]/float(batch_size)\n",
    "    return wavelet_coeff_avg\n",
    "\n",
    "def batch_z_score_transform(batch_lists):\n",
    "    \"\"\"\n",
    "    Takes batch of data, returns the batch with each set as the z-scored data of the original set.\n",
    "    Inputs:\n",
    "        batch_lists (list of lists): the collection of data to be z-scored\n",
    "    Outputs:\n",
    "        A list of lists of the same dimension as the input, but each individual list is the z-scored list of the original data.\n",
    "    \"\"\"\n",
    "    batch_size = len(batch_lists)\n",
    "    z_score_lists = []\n",
    "    for i in range(0, batch_size):\n",
    "        z_score_set = z_score_list(batch_lists[i])\n",
    "        z_score_lists.append(z_score_set)\n",
    "    return z_score_lists\n",
    "\n",
    "def batch_z_score_wavelet_transform(batch_lists):\n",
    "    \"\"\"\n",
    "    Takes raw data, z-scores it, and then applies the Morlet wavelet transform to each set in the batch. For comparing to \n",
    "    the batch wavelet transforms.\n",
    "    Inputs:\n",
    "        batch_lists (list of lists): list of the raw data lists to be z-scored and transformed.\n",
    "    Outputs:\n",
    "        Average wavelet coefficients for the z-scored data.\n",
    "    \"\"\"\n",
    "    z_scored_data = batch_z_score_transform(batch_lists)\n",
    "    return batch_wavelet_transform(z_scored_data)\n",
    "    \n",
    "def z_score_list(data):\n",
    "    \"\"\"\n",
    "    Returns the z-scored data.\n",
    "    \"\"\"\n",
    "    mean = sum(data)/float(len(data))\n",
    "    data_sum = 0\n",
    "    for i in range(0, len(data)):\n",
    "        data_sum += (data[i]-mean)*(data[i]-mean)\n",
    "    std_dev = data_sum/float(len(data))\n",
    "    data_z_score = []\n",
    "    for i in range(0, len(data)):\n",
    "        data_z_score.append((data[i]-mean)/std_dev)\n",
    "    return data_z_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHANGELOG\n",
    "---------\n",
    "\n",
    "3/13/17 \n",
    "- Split from NE Climate Database to condense methods w/o plotting scripts\n",
    "\n",
    "3/29/17\n",
    "- wrote lag-1 autocorrelated data batch generator\n",
    "- wrote z_score_list method and batch_z_score_transform\n",
    "- wrote batch_wavelet_transform\n",
    "\n",
    "4/10/17\n",
    "- adding lat/long/elev values to the StateIDs\n",
    "\n",
    "4/18/17\n",
    "- adding script to sort StateIDs by the Appalachian mountain range\n",
    "- created sub-database with sorted StateIDs"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
