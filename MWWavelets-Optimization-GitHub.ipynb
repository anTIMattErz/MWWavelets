{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NE Climate Data Wavelet Analysis Methods\n",
    "### MWWavelets\n",
    "This file collects all the methods used in applying the wavelet transforms to the database created for the NE Climate wavelet project. It first constructs the NE Climate database, then defines all of the methods for applying the transforms, along with various other helper sections for creating noisy data and adding the COI's on plots. Statistical testing section is included, but is not applicable to the current data set.\n",
    "\n",
    "Examples of using the code are included in the Plotting Scripts notebook.\n",
    "\n",
    "### Contents\n",
    "1. [Constructing the StateID Object](#Construction)\n",
    "2. [Retrieving and Smoothing StateID Data](#Retrieving)\n",
    "3. [Generating Noisy Data](#Noisy)\n",
    "4. [Wavelet Classes](#Wavelet)\n",
    "5. [Statistical Testing](#Statistical)\n",
    "6. [Generating Batches of Random Data](#Batch_Gen)\n",
    "7. [Transforming Batches of Random Data, \"Signal Profile\" Creation](#Batch_Trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "import ast\n",
    "import natsort #used in sorting dictionary keys for dataframes\n",
    "from matplotlib.patches import Ellipse, Polygon\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.interpolate import interp1d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Construction'></a>\n",
    "## 1. Constructing the NE Climate Database\n",
    "------------------------------\n",
    "I collected the data for each weather station into a StateID object. This section defines the StateID class, then uses the database to populate the objects for every station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint 1\n",
      "checkpoint 2\n",
      "Completed Initilization of Objects!\n"
     ]
    }
   ],
   "source": [
    "class StateID:\n",
    "    \"\"\"\n",
    "    See above documentation for StateID class properties.\n",
    "    \"\"\"\n",
    "    winter_months = [10, 11, 0, 1]\n",
    "    spring_months = [2, 3, 4, 5]\n",
    "    summer_months = [6, 7, 8, 9]\n",
    "    \n",
    "    def __init__(self, StateID, MaxTempDict, MinTempDict, PrecipDict):\n",
    "        self._name = StateID\n",
    "        self._max_temp = MaxTempDict\n",
    "        self._min_temp = MinTempDict\n",
    "        self._precip = PrecipDict\n",
    "        \n",
    "        # these will be set later with a different file/not used for anything but filtering stations\n",
    "        self._coord = -9999 # tuple containing (latitude, longitude)\n",
    "        self._elev = -9999\n",
    "        \n",
    "        self._max_temp_winter = {}\n",
    "        self._max_temp_spring = {}\n",
    "        self._max_temp_summer = {}\n",
    "        self._min_temp_winter = {}\n",
    "        self._min_temp_spring = {}\n",
    "        self._min_temp_summer = {}\n",
    "        self._precip_winter = {}\n",
    "        self._precip_spring = {}\n",
    "        self._precip_summer = {}\n",
    "        \n",
    "        self.make_winter_max_temp_dict()\n",
    "        self.make_spring_max_temp_dict()\n",
    "        self.make_summer_max_temp_dict()\n",
    "        self.make_winter_min_temp_dict()\n",
    "        self.make_spring_min_temp_dict()\n",
    "        self.make_summer_min_temp_dict()\n",
    "        self.make_winter_precip_dict()\n",
    "        self.make_spring_precip_dict()\n",
    "        self.make_summer_precip_dict()\n",
    "    \n",
    "    #getter functions\n",
    "    def name(self):\n",
    "        return self._name\n",
    "    def max_temp(self):\n",
    "        return self._max_temp\n",
    "    def min_temp(self):\n",
    "        return self._min_temp\n",
    "    def precip(self):\n",
    "        return self._precip\n",
    "    \n",
    "    #(lat, long) and elev\n",
    "    def coord(self):\n",
    "        return self._coord\n",
    "    def elev(self):\n",
    "        return self._elev\n",
    "    \n",
    "    def max_temp_winter(self):\n",
    "        return self._max_temp_winter\n",
    "    def max_temp_spring(self):\n",
    "        return self._max_temp_spring\n",
    "    def max_temp_summer(self):\n",
    "        return self._max_temp_summer\n",
    "    def min_temp_winter(self):\n",
    "        return self._min_temp_winter\n",
    "    def min_temp_spring(self):\n",
    "        return self._min_temp_spring\n",
    "    def min_temp_summer(self):\n",
    "        return self._min_temp_summer\n",
    "    def precip_winter(self):\n",
    "        return self._precip_winter\n",
    "    def precip_spring(self):\n",
    "        return self._precip_spring\n",
    "    def precip_summer(self):\n",
    "        return self._precip_summer\n",
    "    \n",
    "    #seasonal dictionary builders\n",
    "    def make_winter_max_temp_dict(self):\n",
    "        new_dict = {}\n",
    "        for year in self._max_temp.keys():\n",
    "            new_dict[year] = self._max_temp.get(year)[0:2]\n",
    "            new_dict[year] += self._max_temp.get(year)[10:12]\n",
    "        self._max_temp_winter = new_dict\n",
    "    def make_spring_max_temp_dict(self):\n",
    "        new_dict = {}\n",
    "        for year in self._max_temp.keys():\n",
    "            new_dict[year] = self._max_temp.get(year)[2:6]\n",
    "        self._max_temp_spring = new_dict\n",
    "    def make_summer_max_temp_dict(self):\n",
    "        new_dict = {}\n",
    "        for year in self._max_temp.keys():\n",
    "            new_dict[year] = self._max_temp.get(year)[6:10]\n",
    "        self._max_temp_summer = new_dict\n",
    "    def make_winter_min_temp_dict(self):\n",
    "        new_dict = {}\n",
    "        for year in self._min_temp.keys():\n",
    "            new_dict[year] = self._min_temp.get(year)[0:2]\n",
    "            new_dict[year] += self._min_temp.get(year)[10:12]\n",
    "        self._min_temp_winter = new_dict\n",
    "    def make_spring_min_temp_dict(self):\n",
    "        new_dict = {}\n",
    "        for year in self._min_temp.keys():\n",
    "            new_dict[year] = self._min_temp.get(year)[2:6]\n",
    "        self._min_temp_spring = new_dict\n",
    "    def make_summer_min_temp_dict(self):\n",
    "        new_dict = {}\n",
    "        for year in self._min_temp.keys():\n",
    "            new_dict[year] = self._min_temp.get(year)[6:10]\n",
    "        self._min_temp_summer = new_dict\n",
    "    def make_winter_precip_dict(self):\n",
    "        new_dict = {}\n",
    "        for year in self._precip.keys():\n",
    "            new_dict[year] = self._precip.get(year)[0:2]\n",
    "            new_dict[year] += self._precip.get(year)[10:12]\n",
    "        self._precip_winter = new_dict\n",
    "    def make_spring_precip_dict(self):\n",
    "        new_dict = {}\n",
    "        for year in self._precip.keys():\n",
    "            new_dict[year] = self._precip.get(year)[2:6]\n",
    "        self._precip_spring = new_dict\n",
    "    def make_summer_precip_dict(self):\n",
    "        new_dict = {}\n",
    "        for year in self._precip.keys():\n",
    "            new_dict[year] = self._precip.get(year)[6:10]\n",
    "        self._precip_summer = new_dict\n",
    "        \n",
    "        \n",
    "print(\"checkpoint 1\")\n",
    "with open('womack.csv') as csvfile:\n",
    "    rows = csv.reader(csvfile, delimiter=',')\n",
    "    all_data = [row for row in rows]\n",
    "    \n",
    "#removing the first row to leave just data\n",
    "all_data = all_data[1:]\n",
    "#making StateID, year and month values ints, z-scores floats\n",
    "#\n",
    "for i in all_data:\n",
    "    i[0] = int(i[0])   #StateID\n",
    "    i[1] = int(i[1])   #Year\n",
    "    i[2] = int(i[2])   #Month\n",
    "    i[3] = float(i[3]) #MaxTemp z-score\n",
    "    i[4] = float(i[4]) #MinTemp z-score\n",
    "    i[5] = float(i[5]) #Precip z-score\n",
    "\n",
    "print(\"checkpoint 2\")\n",
    "#looping over the unique StateID list to create dictionaries and initialize objects\n",
    "StateID_objects = []\n",
    "for i in range(137):\n",
    "    ID_max_temp_dict = {}\n",
    "    ID_min_temp_dict = {}\n",
    "    ID_precip_dict = {}\n",
    "    for j in range(i*1380,(i+1)*1380):\n",
    "        #we only check existence of key in the max_temp_dict because processing occurs\n",
    "        #for all of the fields at once. max_temp_dict thus is a valid proxy for keys\n",
    "        current_row = all_data[j]\n",
    "        #cleaning the bad data\n",
    "        #some stations were missing data, but no station missed more than 3 months of data\n",
    "        #from January 1908 to September 2010 (Wilkie 24)\n",
    "        #bad data entries were given values of -9999, replacing them with values of 0 (should not affect analysis)\n",
    "        for k in range(3,6):\n",
    "            if (current_row[k] == -9999.0):\n",
    "                #print(\"Cleaning at ID: \", str(current_row[0]))\n",
    "                #print(\"Year: \"+str(current_row[1])+ \"   Value: \"+str(current_row[k]))\n",
    "                current_row[k] = 0\n",
    "                #print(\"Set to: \", current_row[k])\n",
    "        current_year = current_row[1]\n",
    "        if current_year in ID_max_temp_dict:\n",
    "            ID_max_temp_dict[current_year].append(current_row[3])\n",
    "            ID_min_temp_dict[current_year].append(current_row[4])\n",
    "            ID_precip_dict[current_year].append(current_row[5])\n",
    "        else:\n",
    "            ID_max_temp_dict[current_year] = [current_row[3]]\n",
    "            ID_min_temp_dict[current_year] = [current_row[4]]\n",
    "            ID_precip_dict[current_year] = [current_row[5]]\n",
    "    current_State_ID = all_data[i*1380][0]\n",
    "    StateID_objects.append(StateID(current_State_ID, ID_max_temp_dict, ID_min_temp_dict, ID_precip_dict))\n",
    "    #print(\"Completed Initilization of StateID Object: \", current_State_ID)\n",
    "\n",
    "print(\"Completed Initilization of Objects!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding coordinates and elevation to Station ID Objects\n",
    "Data obtained from http://www.surfacestations.org/ushcn_stationlist.htm, and stations missing from this data set were found at various sources related to https://www.ncdc.noaa.gov/data-access/land-based-station-data/land-based-datasets/us-historical-climatology-network-ushcn.\n",
    "\n",
    "Coordinates will be in a tuple, (latitude, longitude). Elevation is measured in units of feet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished adding coordinates and elevation to StateIDs!\n"
     ]
    }
   ],
   "source": [
    "with open('USHCN_ID_Collection_csv.csv') as csvfile:\n",
    "    rows = csv.reader(csvfile, delimiter=',')\n",
    "    all_data = [row for row in rows]\n",
    "\n",
    "# removing the first row to leave just data\n",
    "all_data = all_data[1:]\n",
    "for i in all_data:\n",
    "    i[0] = int(i[0])   #StateID\n",
    "    i[1] = float(i[1])   #Latitude\n",
    "    i[2] = float(i[2])   #Longitude\n",
    "    i[3] = float(i[3])   #Elevation\n",
    "\n",
    "# adding to StateID classes\n",
    "for i in StateID_objects:\n",
    "    for j in all_data:\n",
    "        if i.name() == j[0]:\n",
    "            i._coord = (j[1], j[2])\n",
    "            i._elev = j[3]\n",
    "            \n",
    "print(\"Finished adding coordinates and elevation to StateIDs!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting to West/East of Appalachian Mountains ###\n",
    "Coordinates of Appalachian mountains were chosen by using Google Maps topography map to track the eastmost \"edge\" of the mountain range. Six points were chose to approximate the extent of the mountain range. appalachian_coord takes the longitude (between -83.72 and -66.51 degrees) and returns the mountain range's corresponding latitude. We sort the StateID_objects into either West or East of the mountain range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of StateIDs West of Appalachians:  83\n",
      "Number of StateIDs East of Appalachians:  54\n"
     ]
    }
   ],
   "source": [
    "def appalachian_coord(longitude):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "            longitude (float): the longitude coordinate\n",
    "    Output:\n",
    "            Returns the latitude (float) of the Appalachian mountain range East edge corresponding to the \n",
    "            given longitude coordinate.\n",
    "    \"\"\"\n",
    "    mount_coords = [(34.50, -83.72), (37.58, -79.37), (40.00, -77.36), (41.01, -74.35), (43.80, -72.12), (47.82, -66.51)]\n",
    "    if longitude < mount_coords[1][1]:\n",
    "        b1 = mount_coords[0][0]\n",
    "        slope1 = (mount_coords[1][0]-mount_coords[0][0])/(mount_coords[1][1]-mount_coords[0][1])\n",
    "        return b1 + slope1*(longitude-mount_coords[0][1])\n",
    "    elif longitude < mount_coords[2][1]:\n",
    "        b2 = mount_coords[1][0]\n",
    "        slope2 = (mount_coords[2][0]-mount_coords[1][0])/(mount_coords[2][1]-mount_coords[1][1])\n",
    "        return b2 + slope2*(longitude-mount_coords[1][1])\n",
    "    elif longitude < mount_coords[3][1]:\n",
    "        b3 = mount_coords[2][0]\n",
    "        slope3 = (mount_coords[3][0]-mount_coords[2][0])/(mount_coords[3][1]-mount_coords[2][1])\n",
    "        return b3 + slope3*(longitude-mount_coords[2][1])\n",
    "    elif longitude < mount_coords[4][1]:\n",
    "        b4 = mount_coords[3][0]\n",
    "        slope4 = (mount_coords[4][0]-mount_coords[3][0])/(mount_coords[4][1]-mount_coords[3][1])\n",
    "        return b4 + slope4*(longitude-mount_coords[3][1])\n",
    "    else:\n",
    "        b5 = mount_coords[4][0]\n",
    "        slope5 = (mount_coords[5][0]-mount_coords[4][0])/(mount_coords[5][1]-mount_coords[4][1])\n",
    "        return b5 + slope5*(longitude-mount_coords[4][1])\n",
    "    \n",
    "def app_west_east(State_ID_object):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "            State_ID_object (StateID_object): the StateID object to consider\n",
    "    Output:\n",
    "            True if the State_ID_object is located West of the Appalachian mountain range,\n",
    "            False if the State_ID_object is East of the mountain range.\n",
    "    \"\"\"\n",
    "    ID_coords = State_ID_object.coord()\n",
    "    app_lat = appalachian_coord(ID_coords[1])\n",
    "    if app_lat < ID_coords[0]:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def app_sorter(ID_obj_list):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "            ID_obj_list (list of StateID_objects): the list of StateID objects to sort\n",
    "    Output:\n",
    "            A 2D list, first list is the StateID objects West of the Appalachian mountain range,\n",
    "            second list is the StateID objects East of the mountain range.\n",
    "    \"\"\"\n",
    "    west_list = []\n",
    "    east_list = []\n",
    "    for i in ID_obj_list:\n",
    "        if app_west_east(i) == True:\n",
    "            west_list.append(i)\n",
    "        else:\n",
    "            east_list.append(i)\n",
    "    return [west_list, east_list]\n",
    "\n",
    "Appalachian_StateID_objects = app_sorter(StateID_objects)\n",
    "print(\"Number of StateIDs West of Appalachians: \", len(Appalachian_StateID_objects[0]))\n",
    "print(\"Number of StateIDs East of Appalachians: \", len(Appalachian_StateID_objects[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Retrieving'></a>\n",
    "## 2. Retrieving and Smoothing StateID Data\n",
    "------------------------------\n",
    "1. The select_id function returns the object corresponding to the given ID.\n",
    "2. The dict_to_list function takes the data dictionaries from the StateID classes and converts them to a list, with the values in chronological order.\n",
    "3. The gaussian function is simply a function that gives the value of the normalized gaussian centered at mu for a specified x value and standard deviation. This is used when we want to use gaussian kernel smoothing.\n",
    "4. The normal_kernel_constructor creates an array of uniformly sampled values of the gaussian function to smooth the data with.\n",
    "5. The gaussian_smoother function takes an array of x-axis data, y-axis data, and a standard deviation, and smooths the y-axis data with a gaussian kernel according to the given standard deviation. It outputs a 2-D array containing the correct truncated x-axis data and smoothed y-axis data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def select_id(id_number, id_obj_list):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "            id_number (int): the ID number of the desired StateID.\n",
    "            id_obj_list (list): the list of StateID objects created in the first part of the code.\n",
    "    Output:\n",
    "            the StateID object with name corresponding to the given id_number.\n",
    "    \"\"\"\n",
    "    for obj in id_obj_list:\n",
    "        if obj.name() == id_number:\n",
    "            return obj\n",
    "    \n",
    "def dict_to_list(dictionary):\n",
    "    converted_list = []\n",
    "    keys = list(dictionary.keys())\n",
    "    keys.sort()\n",
    "    for i in keys:\n",
    "        converted_list += dictionary.get(i)\n",
    "    return converted_list\n",
    "\n",
    "def gaussian(x, mu, sig):\n",
    "    return 1./(np.sqrt(2.*np.pi)*sig)*np.exp(-np.power((x - mu)/sig, 2.)/2)\n",
    "\n",
    "def normal_kernel_constructor(std_dev):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "            std_dev (float): the standard deviation of the kernel to be used, unit of years\n",
    "    NOTE:   std_dev must give an int when multiplied by 12 so that we get an integer\n",
    "            number of sampled points for the kernel!\n",
    "    Output:\n",
    "            list of discretely sampled kernel values corresponding to the normal\n",
    "            distribution, covering 95% of kernel (2 sigma on either side of zero)\n",
    "    \"\"\"\n",
    "    width = 2*std_dev #one sided width\n",
    "    n_points = 2*width*12+1\n",
    "    bins = n_points-1\n",
    "    sampled_points = np.linspace(-width, width, num=n_points, endpoint=True)\n",
    "    sampled_values = []\n",
    "    for i in sampled_points:\n",
    "        x = gaussian(i, 0, std_dev)*2*width/bins\n",
    "        sampled_values.append(x)\n",
    "    total = sum(sampled_values)\n",
    "    renormalize_offset = (1-total)/n_points\n",
    "    for i in range(len(sampled_values)):\n",
    "        sampled_values[i] += renormalize_offset\n",
    "    return sampled_values\n",
    "\n",
    "def gaussian_smoother(x_array, y_array, std_dev):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "            x_values (array_like): an array of points for the x-axis\n",
    "            y_values (array_like): an array of the data to be smoothed\n",
    "            std_dev (float):  the std_dev of the kernel to be used, unit of years. Refer to normal_kernel_constructor.\n",
    "    Output:\n",
    "            returns a 2-D numpy array. 1st array contains the x-values corresponding to the smoothed\n",
    "            y-values. 2nd array contains the smoothed y-data.\n",
    "    NOTE: Assumes that y_array, x_array are same dimension, and that the std_dev given does not make\n",
    "          the gaussian kernel larger than the dimension of the y_array!\n",
    "    NOTE: x-array is truncated at both endpoints since the gaussian window starts at the first\n",
    "          point where it does not extend past the data.\n",
    "    \"\"\"\n",
    "    n_of_width_points = int(2*std_dev*12) #number of points required on each side of mean for kernel\n",
    "    x_array_dimension = len(x_array)\n",
    "    truncated_x_list = list(x_array[n_of_width_points:x_array_dimension-n_of_width_points])\n",
    "    \n",
    "    kernel_values = normal_kernel_constructor(std_dev)\n",
    "    smoothed_y_list = []\n",
    "    for i in range(n_of_width_points, x_array_dimension-n_of_width_points):\n",
    "        smooth_pass = 0\n",
    "        for j in range(len(kernel_values)):\n",
    "            smooth_pass += y_array[i+j-n_of_width_points]*kernel_values[j]\n",
    "        smoothed_y_list.append(smooth_pass)\n",
    "        \n",
    "    return [truncated_x_list, smoothed_y_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index and Averaging Methods\n",
    "\n",
    "These methods generate NEI index data from specified StateID objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nei_index(StateID_obj):\n",
    "    \"\"\"\n",
    "    Returns an list containing the monthly NEI index values for the given StateID obj.\n",
    "    Inputs:\n",
    "            StateID_obj: a StateID object\n",
    "    Outputs:\n",
    "            Returns a 1-D list for the monthly NEI index for the given object.\n",
    "    \"\"\"\n",
    "    max_temp_list = dict_to_list(StateID_obj.max_temp())\n",
    "    min_temp_list = dict_to_list(StateID_obj.min_temp())\n",
    "    precip_list = dict_to_list(StateID_obj.precip())\n",
    "    nei_index_list = []\n",
    "    for i in range(len(max_temp_list)):\n",
    "        nei_value = (max_temp_list[i]+min_temp_list[i]+precip_list[i])/3\n",
    "        nei_index_list.append(nei_value)\n",
    "    return nei_index_list\n",
    "    \n",
    "def average_StateID_series(StateID_obj_list):\n",
    "    \"\"\"\n",
    "    Returns a 4-D list containing the averaged max_temp, min_temp, precip and indexed values for each month\n",
    "    across all of the objects in the list.\n",
    "    Inputs:\n",
    "            StateID_obj_list (list): list of StateID objects to average.\n",
    "    Outputs:\n",
    "            Returns a 4-D list. \n",
    "                average_StateID_series()[0] -> averaged max_temp list\n",
    "                average_StateID_series()[1] -> averaged min_temp list\n",
    "                average_StateID_series()[2] -> averaged precip list\n",
    "                average_StateID_series()[3] -> averaged NEI index list\n",
    "    \"\"\"\n",
    "    N = float(len(StateID_obj_list))\n",
    "    max_temp_lists = []\n",
    "    min_temp_lists = []\n",
    "    precip_lists = []\n",
    "    nei_index_lists = []\n",
    "    for obj in StateID_obj_list:\n",
    "        max_temp = dict_to_list(obj.max_temp())\n",
    "        min_temp = dict_to_list(obj.min_temp())\n",
    "        precip = dict_to_list(obj.precip())\n",
    "        nei = nei_index(obj)\n",
    "        max_temp_lists.append(max_temp)\n",
    "        min_temp_lists.append(min_temp)\n",
    "        precip_lists.append(precip)\n",
    "        nei_index_lists.append(nei)\n",
    "    avg_max_temp_list = list(np.divide(np.sum(max_temp_lists, axis=0), N))\n",
    "    avg_min_temp_list = list(np.divide(np.sum(min_temp_lists, axis=0), N))\n",
    "    avg_precip_list = list(np.divide(np.sum(precip_lists, axis=0), N))\n",
    "    avg_nei_list = list(np.divide(np.sum(nei_index_lists, axis=0), N))\n",
    "    return [avg_max_temp_list, avg_min_temp_list, avg_precip_list, avg_nei_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Noisy'></a>\n",
    "## 3. Generating Noisy Data\n",
    "---------------------\n",
    "\n",
    "The majority of the signal filtering and detection done in the thesis uses Monte Carlo simulations with noisy data. We also explored how various trends in pure noisy data biased wavelet coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curve Fitting ###\n",
    "\n",
    "For determining reasonable parameters to generate simulated noisy data from for the simple signal database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# functions to generate lines, sinusoids, and exponentials\n",
    "def fit_line_generator(x_data, y_int, slope):\n",
    "    \"\"\"\n",
    "    x_data (list): x values for the line. list is assumed to start at zero and be in ascending order.\n",
    "    \"\"\"\n",
    "    y_array = [y_int]\n",
    "    for i in range(len(x_data)-1):\n",
    "        dy = (x_data[i+1]-x_data[i])*slope\n",
    "        y_array.append(y_array[i]+dy)\n",
    "    return y_array\n",
    "\n",
    "def fit_sinusoid_generator(x_data, period, amp):\n",
    "    freq = 2*np.pi/float(period)\n",
    "    y_array_sine = amp*np.sin(freq*x_data)\n",
    "    return y_array_sine\n",
    "\n",
    "def fit_exp_generator(x_data, amp, expfact):\n",
    "    y_array = amp*np.exp(x_data*expfact)\n",
    "    return y_array\n",
    "\n",
    "def fit_qspline(x_data, y_int, aslope, bslope):\n",
    "    \"\"\"\n",
    "    Spline with kink at 1/4 of the x_data (x_data assumed to be uniform).\n",
    "    \"\"\"\n",
    "    kink_index = len(x_data)//4\n",
    "    y_data = [i*aslope+y_int for i in x_data[:kink_index+1]]\n",
    "    b_y_int = y_data[-1]\n",
    "    b_x_data = [i-x_data[kink_index+1] for i in x_data[kink_index+1:]]\n",
    "    return y_data+[i*bslope+y_data[-1] for i in b_x_data]\n",
    "    \n",
    "def fit_hspline(x_data, y_int, aslope, bslope):\n",
    "    \"\"\"\n",
    "    Same as qspline but with kink at 1/2 of the x_data.\n",
    "    \"\"\"\n",
    "    kink_index = len(x_data)//2\n",
    "    y_data = [i*aslope+y_int for i in x_data[:kink_index+1]]\n",
    "    b_y_int = y_data[-1]\n",
    "    b_x_data = [i-x_data[kink_index+1] for i in x_data[kink_index+1:]]\n",
    "    return y_data+[i*bslope+y_data[-1] for i in b_x_data]\n",
    "\n",
    "def fit_tspline(x_data, y_int, aslope, bslope):\n",
    "    \"\"\"\n",
    "    Same as qspline but with kink at 3/4 of the x_data.\n",
    "    \"\"\"\n",
    "    kink_index = 3*len(x_data)//4\n",
    "    y_data = [i*aslope+y_int for i in x_data[:kink_index+1]]\n",
    "    b_y_int = y_data[-1]\n",
    "    b_x_data = [i-x_data[kink_index+1] for i in x_data[kink_index+1:]]\n",
    "    return y_data+[i*bslope+y_data[-1] for i in b_x_data]\n",
    "    \n",
    "def sigmoid(x_data):\n",
    "    \"\"\"\n",
    "    Takes a list of x data and returns the corresponding y-values for a sigmoid.\n",
    "    \"\"\"\n",
    "    return [1/(1+np.exp(-i)) for i in x_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def red_noise_generator(r, std_dev, length):\n",
    "    \"\"\"\n",
    "    Using algorithm described by: http://www.atmos.washington.edu/~breth/classes/AM582/lect/lect8-notes.pdf\n",
    "    \n",
    "    Inputs:\n",
    "            r (float): the lag-1 autocorrelation coefficient\n",
    "            std_dev (float): the standard deviation for gaussian white noise used to generate the red noise sequence\n",
    "            length (int): length of the sequence to be generated\n",
    "            \n",
    "    Output:\n",
    "            list of floats for the red noise sequence\n",
    "    \"\"\"\n",
    "    white_noise_sequence = np.random.normal(0, std_dev, length)\n",
    "    white_noise_sequence = white_noise_sequence.tolist()\n",
    "    red_noise_sequence = [white_noise_sequence[0]]\n",
    "    for i in range(1, len(white_noise_sequence)):\n",
    "        next_red_noise = r*red_noise_sequence[i-1] + np.sqrt(1-r*r)*white_noise_sequence[i]\n",
    "        red_noise_sequence.append(next_red_noise)\n",
    "    return red_noise_sequence\n",
    "\n",
    "# noisy line, sinusoid, and exponential generators, 8/9/17\n",
    "def noisy_line(x_data, r, std_dev, y_int, slope):\n",
    "    \"\"\"\n",
    "    Generates a noisy line from the given parameters. \n",
    "    NOTES: ASSUMES THAT X STARTS FROM ZERO!\n",
    "    Inputs:\n",
    "            x_data (array-like): the x points\n",
    "            r (float): the lag-1 autocorrelation coefficient\n",
    "            std_dev (float): the standard deviation for gaussian white noise used to generate the red noise sequence\n",
    "            y_int (float): the y-intercept of the line\n",
    "            slope (float): the slope of the line\n",
    "            \n",
    "    Outputs:\n",
    "            list containing the y-data.\n",
    "    \"\"\"\n",
    "    length = len(x_data)\n",
    "    noise_data = red_noise_generator(r, std_dev, length)\n",
    "    y_data = fit_line_generator(x_data, y_int, slope)\n",
    "    noisy_y = [y_data[i]+noise_data[i] for i in range(length)]\n",
    "    return noisy_y\n",
    "\n",
    "def noisy_sine(x_data, r, std_dev, period, amp):\n",
    "    \"\"\"\n",
    "    Generates a noisy sinusoidal signal from the given parameters.\n",
    "    Inputs:\n",
    "            x_data (array-like): the x points\n",
    "            r (float): the lag-1 autocorrelation coefficient\n",
    "            std_dev (float): the standard deviation for gaussian white noise used to generate the red noise sequence\n",
    "            period (float): the period of the sine signal\n",
    "            amp (float): the amplitude of the sine signal\n",
    "    \n",
    "    Outputs:\n",
    "            list containing the y-data.\n",
    "    \"\"\"\n",
    "    length = len(x_data)\n",
    "    noise_data = red_noise_generator(r, std_dev, length)\n",
    "    y_data = fit_sinusoid_generator(x_data, period, amp)\n",
    "    noisy_y = [y_data[i]+noise_data[i] for i in range(length)]\n",
    "    return noisy_y\n",
    "    \n",
    "def noisy_exp(x_data, r, std_dev, amp, expfact):\n",
    "    \"\"\"\n",
    "    Generates a noisy exponential signal from the given parameters.\n",
    "    Inputs:\n",
    "            x_data (array-like): the x points\n",
    "            r (float): the lag-1 autocorrelation coefficient\n",
    "            std_dev (float): the standard deviation for gaussian white noise used to generate the red noise sequence\n",
    "            amp (float): amplitude of the exponential\n",
    "            expfact (float): factor in the exponent\n",
    "    \n",
    "    Outputs:\n",
    "            list containing the y-values.\n",
    "    \"\"\"\n",
    "    length = len(x_data)\n",
    "    noise_data = red_noise_generator(r, std_dev, length)\n",
    "    y_data = fit_exp_generator(x_data, period, amp)\n",
    "    noisy_y = [y_data[i]+noise_data[i] for i in range(length)]\n",
    "    return noisy_y\n",
    "\n",
    "# you get the idea by now    \n",
    "def noisy_qspline(x_data, r, stdd, y_int, aslope, bslope):\n",
    "    N = len(x_data)\n",
    "    noise_data = red_noise_generator(r, stdd, N)\n",
    "    y_data = fit_qspline(x_data, y_int, aslope, bslope)\n",
    "    return [y_data[i]+noise_data[i] for i in range(N)]\n",
    "\n",
    "def noisy_hspline(x_data, r, stdd, y_int, aslope, bslope):\n",
    "    N = len(x_data)\n",
    "    noise_data = red_noise_generator(r, stdd, N)\n",
    "    y_data = fit_hspline(x_data, y_int, aslope, bslope)\n",
    "    return [y_data[i]+noise_data[i] for i in range(N)]\n",
    "\n",
    "def noisy_tspline(x_data, r, stdd, y_int, aslope, bslope):\n",
    "    N = len(x_data)\n",
    "    noise_data = red_noise_generator(r, stdd, N)\n",
    "    y_data = fit_tspline(x_data, y_int, asplope, bslope)\n",
    "    return [y_data[i]+noise_data[i] for i in range(N)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Wavelet'></a>\n",
    "## 4. Wavelets\n",
    "--------\n",
    "Each different wavelet is its own class that encapsulates the following methods:\n",
    "1. Scaling\n",
    "2. Sampling\n",
    "3. Renormalization\n",
    "\n",
    "Each wavelet takes a float, defining the scale parameter of the wavelet, and calculates the number of sampled points allowed by this scale. Note that the number of sampled points will be odd, so that the central value has an equal number of points on either side for convolution. The wavelet is then uniformly sampled depending on the number of allowed sampling points. Since sampling provides only an approximation of the wavelet, we renormalize the sampled wavelet points so that the wavelet power is equal to one. This is done by numerically integrating the squared sampled wavelet points, subtracting the result from 1, averaging the difference over the number of points, and subtracting the averaged difference from each point. This is similar to renormalizing the sampled values of a gaussian PDF to preserve signal amplitude for smoothing purposes.\n",
    "\n",
    "5/16/2018 NOTE: This renormalization method may not be numerically sound (may bias the sampled wavelet coefficients, smoothing the wavelet shape), but in practice for this work the renormalization required was minimal. However, this package is certainly not reliable for precise analyses.\n",
    "\n",
    "Implemented wavelets:\n",
    "1. Morlet (complex)\n",
    "2. Harr (real)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Morlet Wavelet ####\n",
    "Defined in notes. Also have derivations of scaling methods and normalizing coefficients in notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Morlet_Wavelet:  \n",
    "    def __init__(self, scale, f):\n",
    "        self._scale = scale\n",
    "        self._f = f\n",
    "    def f(self):\n",
    "        return self._f\n",
    "    def scale(self):\n",
    "        return self._scale\n",
    "        \n",
    "    def morlet_function(self):\n",
    "        # same as above but uses numpy vector functions rather than individually generating list elements\n",
    "        x = np.linspace(-2, 2, 2*self.sampling_points()+1)\n",
    "        norm_coeff = np.power(np.pi, -1/4)\n",
    "        complex_array = [-np.complex(0,1)*self.f()*i for i in x]\n",
    "        neg_square_list = [-i*i/2 for i in x]\n",
    "        sampled_values = list(norm_coeff*np.exp(complex_array)*np.exp(neg_square_list))\n",
    "        renorm_sampled_values = self.renormalize(sampled_values)\n",
    "        final_sampled_values = [(1/np.sqrt(self._scale))*i for i in renorm_sampled_values]\n",
    "        return final_sampled_values\n",
    "\n",
    "    def scale_to_year(self):\n",
    "        \"\"\"\n",
    "        Calculates the scale to year conversion for a given frequency and scale.\n",
    "        Outputs:\n",
    "                Returns the number of years the scale represents, float.\n",
    "        \"\"\"\n",
    "        scale_factor = 4*np.pi/(self.f()+np.sqrt(2+self.f()*self.f()))\n",
    "        return scale_factor*self.scale()\n",
    "    \n",
    "    def sampling_points(self):\n",
    "        \"\"\"\n",
    "        Calculates the number of points to sample the wavelet with on one side (tail).\n",
    "        Outputs:\n",
    "                Returns the number of one sided sampling points, int.\n",
    "        \"\"\"\n",
    "        years_for_scale = self.scale_to_year()\n",
    "        one_sided_width = 2*years_for_scale # 2*scale corresponds to 95% of gaussian envelope [see notes]\n",
    "        one_sided_months = 12*one_sided_width\n",
    "        one_sided_points = np.ceil(one_sided_months)\n",
    "        return one_sided_points\n",
    "    \n",
    "    def renormalize(self, sampled_values):\n",
    "        \"\"\"\n",
    "        Renormalization helper function for morlet_function.\n",
    "        Sets total power of sampled wavelet coefficients to 1.\n",
    "        \"\"\"\n",
    "        # NOTE: It turns out that even for small scales the wavelet power is very close to 1\n",
    "        # -> renormalization may not be necessary\n",
    "        power = 0\n",
    "        d_power = 4/(2*self.sampling_points())\n",
    "        renorm_values = []\n",
    "        \n",
    "        ab_values = np.absolute(sampled_values)\n",
    "        power = np.sum(ab_values*ab_values*d_power)        \n",
    "        power_scale_factor = np.sqrt(1/power)\n",
    "        \n",
    "        for i in sampled_values:\n",
    "            renorm_values.append(i*power_scale_factor)\n",
    "        return renorm_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Harr Wavelet ####\n",
    "Scaling method and normalization are trivial. Scale directly corresponds to the year width of the wavelet. Notes on the Harr wavelet are found for 10/15.\n",
    "\n",
    "The Harr wavelet corresponds to a difference in averages. The wavelet coefficient is the difference in the average of the first half of the data within the support of the wavelet, and the average of the second half of the data within the support. A positive value corresponds to the first half having a higher average, while a negative value corresponds to the second half having a higher average.\n",
    "\n",
    "NOTE: I decided to implement the Harr wavelet so that the 1st value of the wavelet corresponds to the current value in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Harr_Wavelet:\n",
    "    \"\"\"\n",
    "    NOTE: SCALE MUST BE A MULTIPLE OF 1/6.\n",
    "    \"\"\"\n",
    "    def __init__(self, scale):\n",
    "        self._scale = scale\n",
    "    def scale(self):\n",
    "        return self._scale\n",
    "    \n",
    "    def harr_function(self):\n",
    "        \"\"\"\n",
    "        Returns a list corresponding to the values of the Harr wavelet for each input value.\n",
    "        These values are already normalized given the scale. Thus, each Harr wavelet class\n",
    "        object corresponds to a daughter wavelet.\n",
    "        Outputs:\n",
    "                Returns the Harr wavelet values at each index, list.\n",
    "        \"\"\"\n",
    "        scale = self.scale()\n",
    "        normalized_value = 1/np.sqrt(12*scale)\n",
    "        first_avg = [normalized_value]*(scale*6)\n",
    "        second_avg = [-1*normalized_value]*(scale*6)\n",
    "        harr_values = first_avg + second_avg\n",
    "        return harr_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution Methods\n",
    "Takes a time series array and a wavelet array, returns the wavelet coefficients for the continuous wavelet transform.\n",
    "\n",
    "Pads the array with zeros depending on COI of given wavelet array. Returned wavelet coefficient array entries correspond to the times in the original time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# MORLET WAVELET\n",
    "# these methods are for standardizing the Morlet wavelet scales\n",
    "def scale_range(scale_min, scale_max):\n",
    "    \"\"\"\n",
    "    Standardizes the granularity of scale ranges.\n",
    "    \"\"\"\n",
    "    return list(np.arange(scale_min, scale_max+0.25, 0.25))\n",
    "\n",
    "def qrtr_scale_range(scale_min, scale_max):\n",
    "    \"\"\"\n",
    "    1/4 of the granularity of scale_range, for making the wavelet transforms of the large simulated data sets\n",
    "    go much faster.\n",
    "    \"\"\"\n",
    "    return list(np.arange(scale_min, scale_max+1, 1))\n",
    "\n",
    "def scale_range_years(scale_list, morlet_freq):\n",
    "    \"\"\"\n",
    "    Returns a list of the corresponding years in each scale for the Morlet wavelet with given frequency.\n",
    "    \"\"\" \n",
    "    year_list = []\n",
    "    scale_factor = 4*np.pi/(morlet_freq+np.sqrt(2+morlet_freq*morlet_freq))\n",
    "    for i in scale_list:\n",
    "        year_list.append(i*scale_factor)\n",
    "    return year_list\n",
    "\n",
    "# Morlet convolution methods\n",
    "def continuous_convolution(time_series, wavelet_list):\n",
    "    # this is convolving one Morlet wavelet with the time series\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "            time_series (list): time series values to be transformed\n",
    "            wavelet_array (list): array of sampled, renormalized values from wavelet\n",
    "            NOTE: wavelet_array must have an odd number of values! (for Morlet wavelet, may need different method\n",
    "            for the Harr wavelet)\n",
    "    Outputs:\n",
    "            Returns a list with the Morlet wavelet coefficients.\n",
    "    \"\"\"\n",
    "    coeffs_list = []\n",
    "    n = len(time_series)\n",
    "    m = len(wavelet_list)\n",
    "    pad_len = int((m-1)/2) #one sided tail length, number of entries to each side of center wavelet entry\n",
    "    padding_list = [0]*pad_len\n",
    "    pad_time_series = np.array(padding_list + time_series + padding_list)\n",
    "    wavelet_list = np.array(wavelet_list)\n",
    "    for i in range(pad_len, pad_len + n):\n",
    "        coeffs_list.append(np.dot(wavelet_list, pad_time_series[i-pad_len:i+pad_len+1]))\n",
    "    return coeffs_list\n",
    "\n",
    "def continuous_transform_morlet(time_series, scale_array, f):\n",
    "    # this is convolving a set of Morlet wavelets with the time series\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "            time_series (array_like): time series values to be transformed\n",
    "            scale_array (array_like): array of values for wavelet scales\n",
    "            f (float): frequency of Morlet wavelet\n",
    "    Outputs:\n",
    "            2D array. Rows are wavelet coefficients for a given scale, columns are wavelet coefficient at each\n",
    "            time series position. 0th index is lowest scale, -1th index is highest scale.\n",
    "    \"\"\"\n",
    "    wavelet_list = []\n",
    "    wavelet_coefficients = []\n",
    "    for i in scale_array:\n",
    "        wvlt = Morlet_Wavelet(i, f)\n",
    "        wavelet_list.append(wvlt.morlet_function())    \n",
    "    for i in wavelet_list:\n",
    "        wavelet_coefficients.append(continuous_convolution(time_series, i))\n",
    "    return wavelet_coefficients\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------------- #\n",
    "# HARR WAVELET\n",
    "# these methods are for standardizing the granularity of the Harr wavelet scales\n",
    "def harr_scale_range(scale_min, scale_max):\n",
    "    \"\"\"\n",
    "    Standardizes the granularity of Harr scale ranges.\n",
    "    Currently set so that each next scale corresponds to one more month of data added to each average.\n",
    "    I.e. two data points are added onto the width of the next wavelet.\n",
    "    \"\"\"\n",
    "    granularity = 1.0/6.0\n",
    "    return list(np.arange(scale_min, scale_max+granularity, granularity))\n",
    "# don't need to convert to years as scale = year for Harr wavelet\n",
    "\n",
    "# these methods are for convolution of the Harr wavelet\n",
    "def harr_convolution(time_series, wavelet_list):\n",
    "    # this is convolving one Harr wavelet with the time series\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "            time_series (array_like): time series values to be transformed\n",
    "            wavelet_array (array_like): array of Harr wavelet values\n",
    "            NOTE: wavelet_array must have an even number of values!\n",
    "    Outputs:\n",
    "            Returns an array with the Harr wavelet coefficients\n",
    "    \"\"\"\n",
    "    coeffs_list = []\n",
    "    n = len(time_series)\n",
    "    m = len(wavelet_list)\n",
    "    pad_len = m #we are only padding the end of the time series due to the Harr wavelet starting at current point\n",
    "    padding_list = [0]*pad_len\n",
    "    pad_time_series = time_series + padding_list\n",
    "    for i in range(0, n):\n",
    "        coeffs_list.append(np.dot(wavelet_list, pad_time_series[i:i+m+1]))\n",
    "    return coeffs_list\n",
    "\n",
    "def continuous_transform_harr(time_series, scale_array):\n",
    "    # this is convolving a set of Harr wavelets with the time series\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "            time_series (array_like): time series values to be transformed\n",
    "            scale_array (array_like): list of values for Harr wavelet scales\n",
    "    Outputs:\n",
    "            ND list. Rows are wavelet coefficients for each given scale, columns are wavelet coefficient at each\n",
    "            time series position.\n",
    "    \"\"\"\n",
    "    wavelet_list = []\n",
    "    wavelet_coefficients = []\n",
    "    for i in scale_array:\n",
    "        wvlt = Harr_Wavelet(i)\n",
    "        wavelet_list.append(wvlt.harr_function())    \n",
    "    for i in wavelet_list:\n",
    "        wavelet_coefficients.append(harr_convolution(time_series, i))\n",
    "    return wavelet_coefficients\n",
    "    \n",
    " \n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------------- #\n",
    "# these methods are for computing the power of the wavelet coefficients\n",
    "def wavelet_power_spectra(wavelet_coefficients):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "            wavelet_coefficients (2D List): 2D list of complex wavelet coefficients. Rows are coefficients for each scale\n",
    "    Output:\n",
    "            2D list of the power of each wavelet coefficient. Rows are squared magnitudes at each scale\n",
    "    \"\"\"\n",
    "    power_spectra_list = []\n",
    "    for i in wavelet_coefficients:\n",
    "        power_spectra_list.append(list(np.square(np.absolute(i))))\n",
    "    return power_spectra_list\n",
    "\n",
    "def normalized_wavelet_power_spectra(wavelet_spectra, variance):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "            wavelet_spectra (2D List): the power of each wavelet coefficient. Rows are squared magnitudes at each scale\n",
    "    Output:\n",
    "            2D list of the normalized power of each wavelet coefficient. Rows are squared normalized magnitude at each scale\n",
    "    \"\"\"\n",
    "    normalized_spectra = []\n",
    "    for i in range(len(wavelet_spectra)):\n",
    "        normalized_spectra_row = []\n",
    "        for j in range(len(wavelet_spectra[0])):\n",
    "            normalized_spectra_row.append((wavelet_spectra[i][j])/variance)\n",
    "        normalized_spectra.append(normalized_spectra_row)\n",
    "    return normalized_spectra\n",
    "\n",
    "def wavelet_power_smoothing(x_data, wavelet_power_list, std_dev):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "            x_data (list): the time coordinates of the data\n",
    "            wavelet_power_list (list): the 2D list of wavelet power coefficients, rows correspond to the scale\n",
    "            std_dev (float): the standard deviation of the gaussian kernel to smooth with, units of years\n",
    "    Output:\n",
    "            2D list, 1st entry is truncated x_data, 2nd entry is 2D list corresponding to the smoothed wavelet power\n",
    "            spectra at each scale\n",
    "    \"\"\"\n",
    "    smoothed_wavelet_power = []\n",
    "    for scale_row in wavelet_power_list:\n",
    "        new_data = gaussian_smoother(x_data, scale_row, std_dev)\n",
    "        smoothed_wavelet_power.append(new_data[1])\n",
    "    return [new_data[0], smoothed_wavelet_power]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Cone of Influence to plots\n",
    "\n",
    "These functions calculate the coordinates for the patches used to show the COI for each wavelet on the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def coi_left(x_data_start, period_range):\n",
    "    \"\"\"\n",
    "    Calculates the COI line for the left side of the COI of the Morlet wavelet.\n",
    "    Inputs:\n",
    "            x_data_start (float): the starting time coordinate of the time series\n",
    "            period_range (list): the periods of the wavelets, in units of years\n",
    "    Outputs:\n",
    "            List of 2D coordinates for the polygon that corresponds to the COI\n",
    "    \"\"\"\n",
    "    coi_x_data = [x_data_start]\n",
    "    coi_x_data.append(2*period_range[0]+x_data_start)\n",
    "    coi_x_data.append(2*period_range[-1]+x_data_start)\n",
    "    coi_x_data.append(x_data_start)\n",
    "    coi_y_data = [period_range[0], period_range[0], period_range[-1], period_range[-1]]\n",
    "    coords = zip(coi_x_data, coi_y_data)\n",
    "    coords_list = []\n",
    "    for i in coords:\n",
    "        coords_list.append(list(i))\n",
    "    return coords_list\n",
    "\n",
    "def coi_right(x_data_end, period_range):\n",
    "    \"\"\"\n",
    "    Calculates the COI line for the left side of the COI of the Morlet wavelet.\n",
    "    Inputs:\n",
    "            x_data_start (float): the starting time coordinate of the time series\n",
    "            period_range (list): the periods of the wavelets, in units of years\n",
    "    Outputs:\n",
    "            List of 2D coordinates for the polygon that corresponds to the COI\n",
    "    \"\"\"\n",
    "    coi_x_data = [x_data_end, x_data_end, x_data_end-(2*period_range[-1]), x_data_end-(2*period_range[0])]\n",
    "    coi_y_data = [period_range[0], period_range[-1], period_range[-1], period_range[0]]\n",
    "    coords = zip(coi_x_data, coi_y_data)\n",
    "    coords_list = []\n",
    "    for i in coords:\n",
    "        coords_list.append(list(i))\n",
    "    return coords_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Statistical'></a>\n",
    "## 5. Statistical Testing\n",
    "--------\n",
    "NOTE: This statistical testing algorithm has a problem where the amplitude of the wavelet coefficients is about an order of magnitude larger than expected. Do not know if this is due to the wavelet implementation, but currently the statistical testing algorithm implemented in Torrence and Compo does not work here.\n",
    "\n",
    "\n",
    "UPDATE 3/12/17: The statistical testing fails not because the methods are incorrect, but because the data does not match the assumptions made by Torrence and Compo. In particular, they assume that the original data is well approximated by a red-noise lag-1 autocorrelated random process. In this case, because every value is the z-score of the original data taken with the mean of the entire data set, every value is highly correlated to every other value. This means that the data will NOT have a power spectrum close to that of red noise, and the statistical testing method fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# taken from: http://passel.unl.edu/pages/informationmodule.php?idinformationmodule=1130447119&topicorder=8&maxto=16&minto=1\n",
    "chi_squared_two_95 = 5.99\n",
    "#TODO - Optimize code below here\n",
    "\n",
    "def autocorrelation(n, time_series):\n",
    "    \"\"\"\n",
    "    Calculates the autocorrelation at lag n. Assumes time_series length is long enough to use approximation of\n",
    "    autocorrelation.\n",
    "    Inputs:\n",
    "            n (int): the lag value for the autocorrelation coefficient to be computed at\n",
    "            time_series (list): the time series data\n",
    "    Outputs:\n",
    "            Returns the autocorrelation coefficient at lag n, float.\n",
    "    \"\"\"\n",
    "    mean = sum(time_series)/float(len(time_series))\n",
    "    numerator = 0\n",
    "    denominator = 0\n",
    "    for i in range(len(time_series)-n):\n",
    "        numerator += (time_series[i]-mean)*(time_series[i+n]-mean)\n",
    "    for i in range(len(time_series)):\n",
    "        denominator += (time_series[i]-mean)*(time_series[i]-mean)\n",
    "    return numerator/denominator\n",
    "\n",
    "def sample_variance(time_series):\n",
    "    \"\"\"\n",
    "    Calculates the unbiased sample variance of the time series.\n",
    "    Inputs:\n",
    "            time_series (list): the time series that we wish to find the variance of\n",
    "    Outputs:\n",
    "            Returns the unbiased sample variance of the time series, float.\n",
    "    \"\"\"\n",
    "    mean = sum(time_series)/len(time_series)\n",
    "    var_sum = 0\n",
    "    for i in range(len(time_series)):\n",
    "        var_sum += (time_series[i]-mean)*(time_series[i]-mean)\n",
    "    return var_sum/(len(time_series)-1)\n",
    "\n",
    "def theory_wavelet_spectra(alpha, period, N):\n",
    "    \"\"\"\n",
    "    Calculates the theoretical normalized wavelet power for a wavelet of given Fourier period, assuming a red noise\n",
    "    process of length N and lag-1 autocorrelation alpha.\n",
    "    Inputs:\n",
    "            alpha (float): the assumed lag-1 autocorrelation value of the time series\n",
    "            period (float): the Fourier period of the wavelet of interest\n",
    "                            NOTE: PERIOD UNIT MUST BE IN MONTHS!\n",
    "            N (integer): the number of values in the time series\n",
    "    Outputs:\n",
    "            Returns the theoretical normalized wavelet power.\n",
    "    \"\"\"\n",
    "    k = N/period\n",
    "    P = (1-alpha*alpha)/(1+alpha*alpha-2*alpha*np.cos(2*np.pi*k/N))\n",
    "    return P\n",
    "\n",
    "def theory_95_confidence_spectra(alpha, period, N):\n",
    "    \"\"\"\n",
    "    Same as theory_wavelet_spectra, but returns the theoretical 95% confidence wavelet power threshold.\n",
    "    NOTE: For complex wavelet only, since complex wavelets have 2 DOF.\n",
    "    \"\"\"\n",
    "    k = N/period\n",
    "    P = (1-alpha*alpha)/(1+alpha*alpha-2*alpha*np.cos(2*np.pi*k/N))\n",
    "    return (1/2)*P*chi_squared_two_95\n",
    "\n",
    "def base_power_subtraction(wavelet_power_spectrum, year_range, alpha):\n",
    "    \"\"\"\n",
    "    Removes the theoretical 95% confidence spectra from the wavelet power spectra. Anything below 0 is set to 0.\n",
    "    The non-zero data thus represents the statistically significant peaks in the data.\n",
    "    Inputs:\n",
    "            wavelet_power_spectrum (2D list): the data containing the wavelet power for the time series. Rows should\n",
    "                                              correspond to different scales\n",
    "            year_range (list): the year values for each scale, used to calculate the theoretical spectra at each scale\n",
    "            alpha (float): the lag-1 autocorrelation of the time series\n",
    "    Outputs:\n",
    "            2D List containing the theoretical 95% confidence spectra subtracted from the wavelet power spectra.\n",
    "    NOTE: The year_range must match the scale range of wavelet_power_spectra!\n",
    "    \"\"\"\n",
    "    subtracted_spectra = []\n",
    "    N = len(wavelet_power_spectrum[0])\n",
    "    theoretical_spectra = []\n",
    "    for i in year_range:\n",
    "        theoretical_spectra.append(theory_95_confidence_spectra(alpha, i*12, N))\n",
    "    for i in range(len(wavelet_power_spectrum)):\n",
    "        subtracted_year = []\n",
    "        for j in range(len(wavelet_power_spectrum[0])):\n",
    "            new_val = wavelet_power_spectrum[i][j]-theoretical_spectra[i]\n",
    "            if (new_val < 0):\n",
    "                new_val = 0\n",
    "            subtracted_year.append(new_val)\n",
    "        subtracted_spectra.append(subtracted_year)\n",
    "    return subtracted_spectra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Batch_Gen'></a>\n",
    "## 6. Generating Batches of Random Data\n",
    "--------\n",
    "Creating batches of 100 sets of randomly generated time series to calculate approximate expected power distribution of wavelet coefficients for the raw data. The data will then be z-score transformed and the wavelet coefficients will be calculated for every set in the batch, and the average wavelet coefficients of the z-scored data will be compared to the average wavelet coefficients of the raw data to determine if there is a correlation between the two.\n",
    "\n",
    "8/9/17\n",
    "Adding code to generate batches of data with a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def no_baseline_red_noise_batch(r, std_dev, length, batch_size):\n",
    "    \"\"\"\n",
    "    Creates a batch of red noise data with given length, standard deviation for Gaussian noise, and lag-1 autocorrelation r,\n",
    "    that has no added baseline. \n",
    "    Inputs: \n",
    "        r (float): lag-1 autocorrelation for the red noise generator\n",
    "        std_dev (float): standard deviation for Gaussian white-noise\n",
    "        length (int): number of data points to generate\n",
    "        batch_size (int): number of sets of data to generate\n",
    "    Outputs:\n",
    "        N-dim list where each list corresponds to one generated red noise data set of the given length.\n",
    "    \"\"\"\n",
    "    batch_list = []\n",
    "    for i in range(0, batch_size):\n",
    "        batch_list.append(red_noise_generator(r, std_dev, length))\n",
    "    return batch_list\n",
    "\n",
    "# 8/9/17 adding code to generate batches of data with a baseline\n",
    "# these functions take a single set of parameters, next set of functions will take range of parameters to generate species\n",
    "# of batches\n",
    "def line_red_noise_batch(x_data, r, std_dev, y_int, slope, batch_size):\n",
    "    \"\"\"\n",
    "    Creates a batch of red noise data with a linear baseline.\n",
    "    Inputs:\n",
    "            x_data (array-like): the x points\n",
    "            r (float): the lag-1 autocorrelation coefficient\n",
    "            std_dev (float): the standard deviation for gaussian white noise used to generate the red noise sequence\n",
    "            y_int (float): the y-intercept of the line\n",
    "            slope (float): the slope of the line\n",
    "            batch_size (int): the number of random datasets to generate\n",
    "            \n",
    "    Outputs:\n",
    "            list of lists containing the y-data, each row is a different trial.\n",
    "    \"\"\"\n",
    "    batch_list = []\n",
    "    for i in range(0, batch_size):\n",
    "        batch_list.append(noisy_line(x_data, r, std_dev, y_int, slope))\n",
    "    return batch_list\n",
    "\n",
    "def sine_red_noise_batch(x_data, r, std_dev, period, amp, batch_size):\n",
    "    \"\"\"\n",
    "    Creates a batch of red noise data with a sine baseline.\n",
    "    Inputs:\n",
    "            x_data (array-like): the x points\n",
    "            r (float): the lag-1 autocorrelation coefficient\n",
    "            std_dev (float): the standard deviation for gaussian white noise used to generate the red noise sequence\n",
    "            period (float): the period of the sine signal\n",
    "            amp (float): the amplitude of the sine signal\n",
    "            batch_size (int): the number of random datasets to generate\n",
    "            \n",
    "    Outputs:\n",
    "            list of lists containing the y-data, each row is a different trial.\n",
    "    \"\"\"\n",
    "    batch_list = []\n",
    "    for i in range(0, batch_size):\n",
    "        batch_list.append(noisy_sine(x_data, r, std_dev, period, amp))\n",
    "    return batch_list\n",
    "\n",
    "def exp_red_noise_batch(x_data, r, std_dev, amp, expfact, batch_size):\n",
    "    \"\"\"\n",
    "    Creates a batch of red noise data with a sine baseline.\n",
    "    Inputs:\n",
    "            x_data (array-like): the x points\n",
    "            r (float): the lag-1 autocorrelation coefficient\n",
    "            std_dev (float): the standard deviation for gaussian white noise used to generate the red noise sequence\n",
    "            amp (float): amplitude of the exponential\n",
    "            expfact (float): factor in the exponent\n",
    "            batch_size (int): the number of random datasets to generate\n",
    "            \n",
    "    Outputs:\n",
    "            list of lists containing the y-data, each row is a different trial.\n",
    "    \"\"\"\n",
    "    batch_list = []\n",
    "    for i in range(0, batch_size):\n",
    "        batch_list.append(noisy_exp(x_data, r, std_dev, amp, expfact))\n",
    "    return batch_list\n",
    "\n",
    "def qspline_red_noise_batch(x_data, r, stdd, y_int, aslope, bslope, batch_size):\n",
    "    batch_list = []\n",
    "    for i in range(0, batch_size):\n",
    "        batch_list.append(noisy_qspline(x_data, r, stdd, y_int, aslope, bslope))\n",
    "    return batch_list\n",
    "\n",
    "def hspline_red_noise_batch(x_data, r, stdd, y_int, aslope, bslope, batch_size):\n",
    "    batch_list = []\n",
    "    for i in range(0, batch_size):\n",
    "        batch_list.append(noisy_hspline(x_data, r, stdd, y_int, aslope, bslope))\n",
    "    return batch_list\n",
    "\n",
    "def tspline_red_noise_batch(x_data, r, stdd, y_int, aslope, bslope, batch_size):\n",
    "    batch_list = []\n",
    "    for i in range(0, batch_size):\n",
    "        batch_list.append(noisy_tspline(x_data, r , stdd, y_int, aslope, bslope))\n",
    "    return batch_list\n",
    "\n",
    "# generating species batches\n",
    "def line_species_noise_batch(x_data, r_list, std_dev_list, y_int, slope, batch_size):\n",
    "    \"\"\"\n",
    "    Generates \"species\" batches of data, linear baseline with various noise parameters.\n",
    "    Inputs:\n",
    "            Same inputs as for the line noise batch, but now:\n",
    "            r_list (list): list of the r-values to simulate\n",
    "            std_dev_list (list): list of the standard deviations to simulate\n",
    "    \n",
    "    Outputs:\n",
    "            4-dim list. 1st dim is different r-values. 2nd dim is different standard deviations. 3rd dim is\n",
    "            different trials in each batch. 4th dim is the time series for each trial.\n",
    "    Output diagram:\n",
    "            output_list[r-value][std-dev][trial][time series index]\n",
    "    \"\"\"\n",
    "    species_data = []\n",
    "    for r in r_list:\n",
    "        std_dev_vary_list = []\n",
    "        for s in std_dev_list:\n",
    "            batch = line_red_noise_batch(x_data, r, s, y_int, slope, batch_size)\n",
    "            std_dev_vary_list.append(batch)\n",
    "        species_data.append(std_dev_vary_list)\n",
    "    return species_data\n",
    "\n",
    "def sine_species_noise_batch(x_data, r_list, std_dev_list, period, amp, batch_size):\n",
    "    \"\"\"\n",
    "    Generates \"species\" batches of data, sine baseline with various noise parameters.\n",
    "    Inputs:\n",
    "            Same inputs as for the line noise batch, but now:\n",
    "            r_list (list): list of the r-values to simulate\n",
    "            std_dev_list (list): list of the standard deviations to simulate\n",
    "    \n",
    "    Outputs:\n",
    "            4-dim list. 1st dim is different r-values. 2nd dim is different standard deviations. 3rd dim is\n",
    "            different trials in each batch. 4th dim is the time series for each trial.\n",
    "    Output diagram:\n",
    "            output_list[r-value][std-dev][trial][time series index]\n",
    "    \"\"\"\n",
    "    species_data = []\n",
    "    for r in r_list:\n",
    "        std_dev_vary_list = []\n",
    "        for s in std_dev_list:\n",
    "            batch = sine_red_noise_batch(x_data, r, s, period, amp, batch_size)\n",
    "            std_dev_vary_list.append(batch)\n",
    "        species_data.append(std_dev_vary_list)\n",
    "    return species_data\n",
    "\n",
    "def exp_species_noise_batch(x_data, r_list, std_dev_list, amp, expfact, batch_size):\n",
    "    \"\"\"\n",
    "    Generates \"species\" batches of data, exponential baseline with various noise parameters.\n",
    "    Inputs:\n",
    "            Same inputs as for the line noise batch, but now:\n",
    "            r_list (list): list of the r-values to simulate\n",
    "            std_dev_list (list): list of the standard deviations to simulate\n",
    "    \n",
    "    Outputs:\n",
    "            4-dim list. 1st dim is different r-values. 2nd dim is different standard deviations. 3rd dim is\n",
    "            different trials in each batch. 4th dim is the time series for each trial.\n",
    "    Output diagram:\n",
    "            output_list[r-value][std-dev][trial][time series index]\n",
    "    \"\"\"\n",
    "    species_data = []\n",
    "    for r in r_list:\n",
    "        std_dev_vary_list = []\n",
    "        for s in std_dev_list:\n",
    "            batch = exp_red_noise_batch(x_data, r, s, amp, expfact, batch_size)\n",
    "            std_dev_vary_list.append(batch)\n",
    "        species_data.append(std_dev_vary_list)\n",
    "    return species_data\n",
    "\n",
    "def qspline_species_noise_batch(x_data, r_list, stdd_list, y_int, aslope, bslope, batch_size):\n",
    "    species_data = []\n",
    "    for r in r_list:\n",
    "        std_dev_vary_list = []\n",
    "        for s in std_dev_list:\n",
    "            std_dev_vary_list.append(qspline_red_noise_batch(x_data, r, s, y_int, aslope, bslope, batch_size))\n",
    "        species_data.append(std_dev_vary_list)\n",
    "    return species_data\n",
    "\n",
    "def hspline_species_noise_batch(x_data, r_list, stdd_list, y_int, aslope, bslope, batch_size):\n",
    "    species_data = []\n",
    "    for r in r_list:\n",
    "        std_dev_vary_list = []\n",
    "        for s in std_dev_list:\n",
    "            std_dev_vary_list.append(hspline_red_noise_batch(x_data, r, s, y_int, aslope, bslope, batch_size))\n",
    "        species_data.append(std_dev_vary_list)\n",
    "    return species_data\n",
    "\n",
    "def tspline_species_noise_batch(x_data, r_list, stdd_list, y_int, aslope, bslope, batch_size):\n",
    "    species_data = []\n",
    "    for r in r_list:\n",
    "        std_dev_vary_list = []\n",
    "        for s in std_dev_list:\n",
    "            std_dev_vary_list.append(tspline_red_noise_batch(x_data, r, s, y_int, aslope, bslope, batch_size))\n",
    "        species_data.append(std_dev_vary_list)\n",
    "    return species_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Batch_Trans'></a>\n",
    "## 7. Transforming Batches of Random Data\n",
    "----------\n",
    "Applies transform to each set of the given batch. Separate function z-scores the sets in the batch, then applies transform. All functions return the AVERAGE wavelet coefficients for the whole batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_wavelet_transform(batch_lists):\n",
    "    \"\"\"\n",
    "    Takes a list of lists of raw data, returns the average wavelet coefficients for the data set.\n",
    "    Inputs:\n",
    "        batch_lists (list of lists): lists of raw data to be transformed\n",
    "        NOTE: ASSUMES MORE THAN ONE LIST!\n",
    "        NOTE: Assumes that each list is the same length.\n",
    "    Outputs:\n",
    "        List containing average wavelet coefficients.\n",
    "    \"\"\"\n",
    "    wavelet_coeff_unavg = []\n",
    "    batch_size = len(batch_lists)\n",
    "    N = len(batch_lists[0])\n",
    "    wavelet_scales = scale_range(0.5, 30)\n",
    "    M = len(wavelet_scales)\n",
    "    for i in range(0, batch_size):\n",
    "        wavelet_coeff_unavg.append(continuous_transform_morlet(batch_lists[i], wavelet_scales, 2*np.pi))\n",
    "\n",
    "    # now we average the coefficients. wavelet_coefficients_unaveraged is a list of 2-dim lists containing the wavelet\n",
    "    # coefficients. \n",
    "    # creating a list of lists of zero with same dimension as the wavelet coefficients list of lists\n",
    "    wavelet_coeff_avg = []\n",
    "    zero_list = [0]*N\n",
    "    for i in range(0, M):\n",
    "        wavelet_coeff_avg.append(zero_list)\n",
    "    # summing all wavelet values from each data set, averaging by the number of data sets\n",
    "    for i in range(0, batch_size):\n",
    "        for j in range(0, M):\n",
    "            wavelet_coeff_avg[j] = [sum(x) for x in zip(wavelet_coeff_avg[j], wavelet_coeff_unavg[i][j])]\n",
    "    for j in range(0, M):\n",
    "        for k in range(0, N):\n",
    "            wavelet_coeff_avg[j][k] = wavelet_coeff_avg[j][k]/float(batch_size)\n",
    "    return wavelet_coeff_avg\n",
    "\n",
    "def z_score_list(data):\n",
    "    \"\"\"\n",
    "    Returns the z-scored data.\n",
    "    \"\"\"\n",
    "    mean = sum(data)/float(len(data))\n",
    "    data_sum = 0\n",
    "    for i in range(0, len(data)):\n",
    "        data_sum += (data[i]-mean)*(data[i]-mean)\n",
    "    std_dev = data_sum/float(len(data))\n",
    "    data_z_score = []\n",
    "    for i in range(0, len(data)):\n",
    "        data_z_score.append((data[i]-mean)/std_dev)\n",
    "    return data_z_score\n",
    "\n",
    "def batch_z_score_transform(batch_lists):\n",
    "    \"\"\"\n",
    "    Takes batch of data, returns the batch with each set as the z-scored data of the original set.\n",
    "    Inputs:\n",
    "        batch_lists (list of lists): the collection of data to be z-scored\n",
    "    Outputs:\n",
    "        A list of lists of the same dimension as the input, but each individual list is the z-scored list of the original data.\n",
    "    \"\"\"\n",
    "    batch_size = len(batch_lists)\n",
    "    z_score_lists = []\n",
    "    for i in range(0, batch_size):\n",
    "        z_score_set = z_score_list(batch_lists[i])\n",
    "        z_score_lists.append(z_score_set)\n",
    "    return z_score_lists\n",
    "\n",
    "def batch_z_score_wavelet_transform(batch_lists):\n",
    "    \"\"\"\n",
    "    Takes raw data, z-scores it, and then applies the Morlet wavelet transform to each set in the batch. For comparing to \n",
    "    the batch wavelet transforms.\n",
    "    Inputs:\n",
    "        batch_lists (list of lists): list of the raw data lists to be z-scored and transformed.\n",
    "    Outputs:\n",
    "        Average wavelet coefficients for the z-scored data.\n",
    "    \"\"\"\n",
    "    z_scored_data = batch_z_score_transform(batch_lists)\n",
    "    return batch_wavelet_transform(z_scored_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing wavelet transforms of the randomly generated \"species\" data for different baselines. Due to the nonlinearity of taking the wavelet power, it is important that the wavelet transform is applied to each trial of data (and power coefficients calculated first) BEFORE calculating the average wavelet power coefficients. We then save the wavelet power coefficients, along with the power profiles with respect to scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_sim_data(filename):\n",
    "    \"\"\"\n",
    "    Changes CWD to the Noisy_Sim_Data folder, loads the file with the file name into a \n",
    "    2-dim list, then changes the CWD back to the original directory.\n",
    "    \n",
    "    Inputs:\n",
    "        filename (string): the name of the csv file to load\n",
    "    Outputs:\n",
    "        A 2-D list from the specified file.\n",
    "    NOTE: Columns correspond to the trials, rows correspond to points along the time series of each trial.\n",
    "    A slice gives a single trial's time series.\n",
    "    \"\"\"\n",
    "    os.chdir('C:\\\\Anaconda3\\\\WAVELETPROJECT\\\\Noisy_Sim_Data')\n",
    "    df = pd.read_csv(filename)\n",
    "    list_df = df.as_matrix().transpose().tolist()\n",
    "    os.chdir('C:\\\\Anaconda3\\\\WAVELETPROJECT')\n",
    "    return list_df\n",
    "\n",
    "def load_profile(filename):\n",
    "    \"\"\"\n",
    "    Loads profile from saved profiles. Note that this will only be useful for the profiles made from code below.\n",
    "    \"\"\"\n",
    "    os.chdir('C:\\\\Anaconda3\\\\WAVELETPROJECT\\\\Noisy_Sim_Profiles')\n",
    "    df = pd.read_csv(filename)\n",
    "    list_df = df.as_matrix().tolist()\n",
    "    profile_raw = list_df[0]\n",
    "    profile = []\n",
    "    # removing 'profile' column and NaN entries\n",
    "    for i in profile_raw:\n",
    "        if i == 'profile' or isinstance(i, float):\n",
    "            pass\n",
    "        else:\n",
    "            profile.append(ast.literal_eval(i))\n",
    "    os.chdir('C:\\\\Anaconda3\\\\WAVELETPROJECT')\n",
    "    return profile\n",
    "\n",
    "def sim_data_transform(filename, wavelet_scales):\n",
    "    \"\"\"\n",
    "    NOTE: SAME BASIC ALGORITHM AS THE BATCH TRANSFORM.\n",
    "\n",
    "    Takes a file, calculates the average wavelet power coefficients. Note that this is an extremely inefficient\n",
    "    algorithm and will likely take a very long time to complete.\n",
    "    Inputs:\n",
    "        filename (string): path to file to be processed\n",
    "        scale_list (list): list of the scales to perform the wavelet transform at\n",
    "    Outputs:\n",
    "        2-dim list containing the average wavelet power coefficients.\n",
    "    \"\"\"\n",
    "    batch_lists = load_sim_data(filename)\n",
    "    # calculating the wavelet power coefficients for each trial\n",
    "    wavelet_power_unavg = []\n",
    "    batch_size = len(batch_lists)\n",
    "    N = len(batch_lists[0])\n",
    "    M = len(wavelet_scales)\n",
    "    for i in range(0, batch_size):\n",
    "        wavelet_power_unavg.append(wavelet_power_spectra(continuous_transform_morlet(batch_lists[i], wavelet_scales, 2*np.pi)))\n",
    "    # now calculating the average wavelet power coefficients for all the trials\n",
    "    wavelet_power_avg = []\n",
    "    zero_list = [0]*N\n",
    "    for i in range(0, M):\n",
    "        wavelet_power_avg.append(zero_list)\n",
    "    # summing all wavelet values from each data set\n",
    "#NOTE THAT THIS SEEMS EXTREMELY INEFFICIENT HERE\n",
    "    for i in range(0, batch_size):\n",
    "        for j in range(0, M):\n",
    "            wavelet_power_avg[j] = [sum(x) for x in zip(wavelet_power_avg[j], wavelet_power_unavg[i][j])]\n",
    "    # dividing by number of data sets to average\n",
    "    for j in range(0, M):\n",
    "        for k in range(0, N):\n",
    "            wavelet_power_avg[j][k] = wavelet_power_avg[j][k]/float(batch_size)\n",
    "    return wavelet_power_avg\n",
    "\n",
    "def power_profile(wvlt_pwr_matrix, scale_range):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        wvlt_pwr_matrix (2-dim list of lists): the matrix containing the wavelet transform power coefficients\n",
    "        scale_range (1-dim list): the list containing the scales used in the wavelet transform\n",
    "    Outputs:\n",
    "        A list that contains the power profile of the given power coefficients.\n",
    "        Has the form [(scale, avg_power@scale),...]\n",
    "    \"\"\"\n",
    "    profile = []\n",
    "    scale_N = len(scale_range)\n",
    "    series_N = float(len(wvlt_pwr_matrix[0]))\n",
    "    for i in range(scale_N):\n",
    "        avg_pwr = math.fsum(wvlt_pwr_matrix[i])/series_N\n",
    "        profile.append((scale_range[i], avg_pwr))\n",
    "    return profile\n",
    "\n",
    "# need a function to create dataframe with power profile and wavelet power matrix in a dataframe\n",
    "def power_profile_dataframe(wvlt_pwr_matrix, scale_range):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        wvlt_pwr_matrix (2-dim list of lists): the matrix containing the wavelet transform power coefficients\n",
    "        scale_range (1-dim list): the list containing the scales used in the wavelet transform\n",
    "    Outputs:\n",
    "        pandas dataframe with 1st row containing power profile, rest containing original wavelet power coefficient\n",
    "        matrix. The 1st row has NaN's to make the power profile row have the same length as the power coefficient lists.\n",
    "    \"\"\"\n",
    "    profile = power_profile(wvlt_pwr_matrix, scale_range)\n",
    "    N = len(wvlt_pwr_matrix[0])\n",
    "    extended_profile = profile+[math.nan]*(N-len(scale_range))\n",
    "    profile_dict = {'profile': extended_profile}\n",
    "    dict_names = ['profile'] # for sorting the dataframe\n",
    "    order = list(range(0, len(scale_range)+1))\n",
    "    for i in range(len(scale_range)):\n",
    "        name = 'scale_'+str(scale_range[i])\n",
    "        dict_names.append(name)\n",
    "        profile_dict[name] = wvlt_pwr_matrix[i]\n",
    "    df = pd.DataFrame(profile_dict)\n",
    "    new_columns = natsort.natsorted(df.columns.values)\n",
    "    df = df.transpose().reindex(new_columns)\n",
    "    return df\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------------- #\n",
    "# attempting to parallelize the processing of power profile creation\n",
    "# requires a function that we can map() over filenames\n",
    "def file_to_profile(filename, scale_range):\n",
    "    global counter\n",
    "    global iter_start\n",
    "    \n",
    "    # data analysis here\n",
    "    power_matrix = sim_data_transform(filename, scale_range)\n",
    "    df = power_profile_dataframe(power_matrix, scale_range)\n",
    "    save_string = i.strip('.csv')+'_POWER_PROFILE.csv'\n",
    "    df.to_csv(save_string)\n",
    "    \n",
    "    counter += 1\n",
    "    \n",
    "# -------------------------------------------------------------------------------------------------------------- #\n",
    "# need a function to provide the \"distance\" between two power profiles\n",
    "def pow_prof_dist(pow_prof_one, pow_prof_two):\n",
    "    \"\"\"\n",
    "    Takes two power profiles and computes the \"distance\" between them. This is simply the squared magnitude of the \n",
    "    difference between the profile vectors.\n",
    "    NOTE: BOTH POWER PROFILES MUST BE THE SAME LENGTH!\n",
    "    Inputs:\n",
    "        pow_prof_one (power profile): power profile of first signal, in format from the power_profile() function\n",
    "        pow_prof_one (power profile): power profile of second signal, in format from the power_profile() function\n",
    "        POWER PROFILE FORMAT: [(scale, avg_power@scale),...]\n",
    "    Outputs:\n",
    "        The squared magnitude of the difference between the power profile vectors.\n",
    "    \"\"\"\n",
    "    N = len(pow_prof_one)\n",
    "    diff = 0\n",
    "    for i in range(N):\n",
    "        diff += (pow_prof_one[i][1]-pow_prof_two[i][1])*(pow_prof_one[i][1]-pow_prof_two[i][1])\n",
    "    return diff"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
